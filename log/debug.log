10-25 16:47 django.security.DisallowedHost ERROR    Invalid HTTP_HOST header: '127.0.0.1:8000'. You may need to add '127.0.0.1' to ALLOWED_HOSTS.
10-25 16:48 crawler.settings INFO     new token =>F16C6F0B07C042FEB835DD9E0276BA7E0A215477E6EF91A976765A429F954E2E4C2E00768B724429BF92E7E3079EAD2E,AD599152EBFA4498B81B304CF8211177A8D515C7D27CF36049CFBA8C7BE322CF39F77FBAD32046E8AD1CFA4A5E2317E4
10-25 17:29 crawler.settings INFO     new token =>4FBA9CD7267B46168239815B942DD127325E931FE56342B17BD88D21F2A46752A73CE7D4C22845F3B0D1AAE50F7C2662,D2C7115367014857AFE966B2BDA752CF6D1449F043BFFE0BAF7FBCEC00118DEEFCD8258B64BF4C13970847731302A993
10-25 17:29 crawler.settings INFO     helpme accept post info
10-25 17:29 crawler.settings INFO     {'flag': 'HM', 'type': 'SSD need this key', 'idcard': '342623198902145713', 'name': '李测试', 'phone': '15121016778', 'token': 'SQC92017102000001', 'seqid': '1111111111111111'}
10-25 17:29 crawler.settings INFO     {'retcode': -1, 'retmessage': '', 'errMsg': "post data seqId's value is empty"}
10-25 17:30 crawler.settings INFO     helpme accept post info
10-25 17:30 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-25 17:30 crawler.settings INFO     helpme accepted arguments:李测试,342623198902145713,SQC92017102000001,15121016778,
10-25 17:30 crawler.settings INFO     obtain hd_blacklist success
10-25 17:30 crawler.settings INFO     李测试,342623198902145713 => EMW018不良信息查询 insert success
10-25 17:30 crawler.settings INFO     helpme saved success
10-25 17:30 crawler.settings INFO     obtain hd_blacklist success
10-25 17:30 crawler.settings INFO     李测试,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:13 crawler.settings INFO     new token =>1445D3C21976457998F42DEB8FFD3226FE9B2666DBC0E760CCA972DAAECB34E9CBD57A3990DC414BBEFF87B8915876F5,7D875D1509284C95B0B2D6D9D9E50117FC00B5353882675F5FB81AD0D3FA179797542DF14E024D54B77C648A636CEF9E
10-25 18:13 crawler.settings INFO     helpme accept post info
10-25 18:13 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-25 18:13 crawler.settings INFO     helpme accepted arguments:李测试,342623198902145713,SQC92017102000001,15121016778,
10-25 18:13 crawler.settings INFO     obtain hd_blacklist success
10-25 18:13 crawler.settings INFO     李测试,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:13 crawler.settings INFO     helpme saved success
10-25 18:13 crawler.settings INFO     obtain hd_blacklist success
10-25 18:13 crawler.settings INFO     李测试,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:16 crawler.settings INFO     helpme accept post info
10-25 18:16 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-25 18:16 crawler.settings INFO     helpme accepted arguments:李冰冰,342623198902145713,SQC92017102000001,15121016778,
10-25 18:16 crawler.settings INFO     obtain hd_blacklist success
10-25 18:16 crawler.settings INFO     李冰冰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:16 crawler.settings INFO     helpme saved success
10-25 18:16 crawler.settings INFO     obtain hd_blacklist success
10-25 18:16 crawler.settings INFO     李冰冰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:22 crawler.settings INFO     helpme accept post info
10-25 18:22 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-25 18:22 crawler.settings INFO     helpme accepted arguments:张三丰,342623198902145713,SQC92017102000001,15121016778,
10-25 18:22 crawler.settings INFO     obtain hd_blacklist success
10-25 18:22 crawler.settings INFO     张三丰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:22 crawler.settings INFO     helpme saved success
10-25 18:22 crawler.settings INFO     obtain hd_blacklist success
10-25 18:22 crawler.settings INFO     张三丰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:31 crawler.settings INFO     helpme accept post info
10-25 18:31 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-25 18:31 crawler.settings INFO     helpme accepted arguments:张三丰,342623198902145713,SQC92017102000001,15121016778,HM
10-25 18:31 crawler.settings INFO     obtain hd_blacklist success
10-25 18:31 crawler.settings INFO     张三丰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:31 crawler.settings INFO     helpme saved success
10-25 18:31 crawler.settings INFO     obtain hd_blacklist success
10-25 18:31 crawler.settings INFO     张三丰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:45 crawler.settings INFO     helpme accept post info
10-25 18:45 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-25 18:45 crawler.settings INFO     helpme accepted arguments:张三丰,342623198902145713,SQC92017102000001,15121016778,HM
10-25 18:45 crawler.settings INFO     obtain hd_blacklist success
10-25 18:45 crawler.settings INFO     张三丰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     helpme saved success
10-25 18:45 crawler.settings INFO     obtain hd_blacklist success
10-25 18:45 crawler.settings INFO     张三丰,342623198902145713 => EMW018不良信息查询 insert success
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 18:45 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>
10-25 19:28 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 88, in post
    return JsonResponse(ret)
  File "F:\python3\lib\site-packages\django\http\response.py", line 524, in __init__
    'In order to allow non-dict objects to be serialized set the '
TypeError: In order to allow non-dict objects to be serialized set the safe parameter to False.
10-25 19:32 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 89, in post
    return JsonResponse(ret)
  File "F:\python3\lib\site-packages\django\http\response.py", line 524, in __init__
    'In order to allow non-dict objects to be serialized set the '
TypeError: In order to allow non-dict objects to be serialized set the safe parameter to False.
10-25 19:33 appCrawler.views.SSDViewNew INFO     EMW018 不良信息查询 ret=>None,<class 'NoneType'>
10-25 19:33 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 89, in post
    return JsonResponse(ret)
  File "F:\python3\lib\site-packages\django\http\response.py", line 524, in __init__
    'In order to allow non-dict objects to be serialized set the '
TypeError: In order to allow non-dict objects to be serialized set the safe parameter to False.
10-25 23:27 crawler.settings INFO     new token =>EB0C1FD37C624AE387491D259919B909ED674D2670C435C4E051769527A32B321F26D6212F0F4AED9BCA3BC934A7912A,0F9BA71EE20A4D0CAEED56B236A96D402CD8C772464C7B0BBFFB833EB3A91C811CFA313255784914A503535FF1780629
10-25 23:31 crawler.settings INFO     new token =>F07D5DA075ED48C893582917901FA8B2FD799BBB2B8FE08F15DDBC704C72F6F4F53A6AB4D1304C4B877B054589EC2BEB,F23FDE7F1D824EFFBBA9F665E8BD4920C6ADCE405B3C7DF05E805D41EDDDE608575C1E71E3AA4CFE8D194BE8A0511C5A
10-25 23:33 crawler.settings INFO     new token =>5E5D6900BC0341CC9BBC1B45298D70CB4B52A658EBFE877F41C59F1AD74AC5A77862ABD65E2645EA9C44CC4867CD6686,49B54785BB9C4699821E26CBE0C43FF37ABF6911769A94BEA1A080C4ADCDB5311BD52CCBC30C404FA1DF89CAFC2D9345
10-25 23:42 crawler.settings INFO     new token =>07B925B4100741B5A33B1418E1F938F5DCD7D913F4E7AB8D226CB86188EBA620F7AF197F90664540A0C19710796110BB,5C807F075F9C49AA9BBD2257E34D997895DEDD6CB335B9935169BF88ED9064E7831048491C94491DA301C3F527B4A7A3
10-25 23:48 crawler.settings ERROR    del error
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\Get_Token.py", line 56, in deltoken
    t.delete()
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 969, in delete
    (self._meta.object_name, self._meta.pk.attname)
AssertionError: hd_token object can't be deleted because its id attribute is set to None.
10-25 23:48 crawler.settings ERROR    del error
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\Get_Token.py", line 59, in deltoken
    t.delete()
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 969, in delete
    (self._meta.object_name, self._meta.pk.attname)
AssertionError: hd_tokenwebloan object can't be deleted because its id attribute is set to None.
10-25 23:48 crawler.settings INFO     new token =>BE3CC26C4D7446F785449656F9503F40F3DC142F4E7E5878D150D588C4F0C2C1831D4768EB96439093843E5B136F02C0,1ED8523813AD40AF901F23E7F731C6AE3404B0C17759DFEED03DE06AED55CD57C3D0E86E84DB4EEFA2CF2F7B66CB18C3
10-25 23:48 crawler.settings INFO     hd_token insert a new token BE3CC26C4D7446F785449656F9503F40F3DC142F4E7E5878D150D588C4F0C2C1831D4768EB96439093843E5B136F02C0,effective 2017/10/26 1:48:05
10-25 23:48 crawler.settings INFO     hd_tokenwebloan insert a new token 1ED8523813AD40AF901F23E7F731C6AE3404B0C17759DFEED03DE06AED55CD57C3D0E86E84DB4EEFA2CF2F7B66CB18C3,effective 2017/10/26 1:48:05
10-25 23:53 crawler.settings INFO     hd_token delete old token
10-25 23:53 crawler.settings INFO     hd_tokenwebloan delete old token
10-25 23:53 crawler.settings INFO     hd_token insert a new token AFBC3AD35BDF455381D22E45093257507647FD1FFB90296BC8C16716887392F81097C3B27C084D3091292300B06A3BFE,effective 2017/10/26 1:53:13
10-25 23:53 crawler.settings INFO     hd_tokenwebloan insert a new token 180D7525ACE04364BECD2BA3E991561778DFECDC37EFAB23CBB2D68443B08F66B6F13F7BCF8847B2987347AB7F14B4FF,effective 2017/10/26 1:53:13
10-26 00:00 crawler.settings ERROR    del error
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\Get_Token.py", line 55, in deltoken
    hd_token.objects.all().delete(using="ssd")
TypeError: delete() got an unexpected keyword argument 'using'
10-26 00:00 crawler.settings ERROR    del error
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\Get_Token.py", line 57, in deltoken
    hd_tokenwebloan.objects.all().delete(using="ssd")
TypeError: delete() got an unexpected keyword argument 'using'
10-26 00:00 crawler.settings INFO     hd_token insert a new token 248F074B2EA0469292804B0483D10AAC2239939F94E29EB20B664DE86C108FB0335ED72CB18D4EA3A7C0AE8549C59EB4,effective 2017/10/26 2:00:29
10-26 00:00 crawler.settings INFO     hd_tokenwebloan insert a new token 0257C718AA8D4890A7B502DAB4A1B04C6F137DBBEAF0802D81037D8A65E7B0C7FA71EB3D005945778046246DE220E17E,effective 2017/10/26 2:00:29
10-26 00:01 crawler.settings ERROR    del error
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\Get_Token.py", line 56, in deltoken
    t.delete(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 969, in delete
    (self._meta.object_name, self._meta.pk.attname)
AssertionError: hd_token object can't be deleted because its id attribute is set to None.
10-26 00:01 crawler.settings ERROR    del error
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\Get_Token.py", line 59, in deltoken
    t.delete(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 969, in delete
    (self._meta.object_name, self._meta.pk.attname)
AssertionError: hd_tokenwebloan object can't be deleted because its id attribute is set to None.
10-26 00:01 crawler.settings INFO     hd_token insert a new token 6FF0A71AAF9E451CA445CBDF5254EF677894BC6B42DB32DD13584A757F9B7D30A22C0D287EB6407BB3BE74FB77FDAAD1,effective 2017/10/26 2:01:45
10-26 00:01 crawler.settings INFO     hd_tokenwebloan insert a new token AC3A8FB6C7D44FBDBBEB3D28A672254D59654A9FB8D50E82973E7809388A96D958FFA49B1D4D44BE92B0921A04353135,effective 2017/10/26 2:01:45
10-26 00:04 crawler.settings INFO     hd_token delete old token
10-26 00:04 crawler.settings INFO     hd_tokenwebloan delete old token
10-26 00:04 crawler.settings INFO     hd_token insert a new token C4AB40D4BD4B4B28B75D54E0F654656BDF094E4E6DD1E5DCA18168D29551B3DC3639BAE3A3B641AFB058BBA404DCC818,effective 2017/10/26 2:04:13
10-26 00:04 crawler.settings INFO     hd_tokenwebloan insert a new token C78E7AFD33E04C85A42C3068ABDA11720781E6EB7D0042E3F84F3500954F2AC782F0836506124109A16D6BDBFEF6C76B,effective 2017/10/26 2:04:13
10-26 00:06 crawler.settings INFO     hd_token delete old token
10-26 00:06 crawler.settings INFO     hd_tokenwebloan delete old token
10-26 00:06 crawler.settings INFO     hd_token insert a new token 78FC5EC4C80C467B8E3ACA0590F4780D509E9FD5B1A98E9BE307F7EA9714FC4399188939A17A4CE18DCD130ED333D3BD,effective 2017/10/26 2:06:17
10-26 00:06 crawler.settings INFO     hd_tokenwebloan insert a new token BE8393ABF349460CBD230A6D4AAE90768DBC9044CE3298374882FE31E1C6DE86214C28FCB615464AB4A45B62EA01422B,effective 2017/10/26 2:06:17
10-26 00:06 crawler.settings INFO     helpme accept post info
10-26 00:06 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 00:06 crawler.settings INFO     helpme accepted arguments:张三丰,342623198902145713,SQC92017102000001,15121016778,HM
10-26 00:06 django.request ERROR    Internal Server Error: /hd-helpme/
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\core\handlers\exception.py", line 41, in inner
    response = get_response(request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "F:\crawler\appCrawler\views\helpMe.py", line 101, in post
    hdsave.main()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 361, in main
    self.hd_blackList()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 131, in hd_blackList
    if self.flag == self.BSDFLAG:
AttributeError: 'hd_interface' object has no attribute 'flag'
10-26 00:10 crawler.settings INFO     helpme accept post info
10-26 00:10 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 00:10 crawler.settings INFO     helpme accepted arguments:零次方,342623198902145713,SQC92017102000001,15121016778,HM
10-26 00:10 crawler.settings INFO     HM 零次方,342623198902145713 insert into success
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     helpme saved success
10-26 00:10 crawler.settings INFO     HM 零次方,342623198902145713 insert into success
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:10 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 00:17 appCrawler.views.SSDViewNew ERROR    EMW018 不良信息查询
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1406, "Data too long for column 'ctime' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 109, in post
    hd.hd_illegal_info()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 216, in hd_illegal_info
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1406, "Data too long for column 'ctime' at row 1")
10-26 00:27 crawler.settings INFO     SSD 零次方,342623198902145713 insert into success
10-26 00:29 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.OperationalError: (1054, "Unknown column 'p_person_id' in 'field list'")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 55, in post
    hd.hd_blackList()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 165, in hd_blackList
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.OperationalError: (1054, "Unknown column 'p_person_id' in 'field list'")
10-26 00:32 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.OperationalError: (1054, "Unknown column 'model_s' in 'field list'")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 55, in post
    hd.hd_blackList()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 164, in hd_blackList
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.OperationalError: (1054, "Unknown column 'model_s' in 'field list'")
10-26 00:36 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.OperationalError: (1054, "Unknown column 'key_id' in 'field list'")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 55, in post
    hd.hd_blackList()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 164, in hd_blackList
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.OperationalError: (1054, "Unknown column 'key_id' in 'field list'")
10-26 00:39 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.OperationalError: (1054, "Unknown column 'key_id' in 'field list'")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 55, in post
    hd.hd_blackList()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 164, in hd_blackList
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.OperationalError: (1054, "Unknown column 'key_id' in 'field list'")
10-26 00:43 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.OperationalError: (1054, "Unknown column 'key_id' in 'field list'")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 55, in post
    hd.hd_blackList()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 164, in hd_blackList
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.OperationalError: (1054, "Unknown column 'key_id' in 'field list'")
10-26 00:44 appCrawler.views.SSDViewNew ERROR    
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1406, "Data too long for column 'ctime' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 55, in post
    hd.hd_blackList()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 164, in hd_blackList
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1406, "Data too long for column 'ctime' at row 1")
10-26 01:13 crawler.settings INFO     SSD 零次方,342623198902145713 insert into success
10-26 01:15 crawler.settings INFO     SSD 零次方,342623198902145713 insert into success
10-26 01:18 crawler.settings INFO     SSD 零次方,342623198902145713 insert into success
10-26 01:24 crawler.settings INFO     hd_token delete old token
10-26 01:24 crawler.settings INFO     hd_tokenwebloan delete old token
10-26 01:24 crawler.settings INFO     hd_token insert a new token 674D4127CD424F6995ABECCF7788A0B6FDF3541192289982371A01537F80DE5A46E5758EDC7045DF9B19D98D53345772,effective 2017/10/26 3:24:00
10-26 01:24 crawler.settings INFO     hd_tokenwebloan insert a new token FD4F6B926BAF4A24A5CAEBB4BA7E460B9181C238C920EE4BDFAFE337285E24CD261D270152734E6E845B9CA093729E2B,effective 2017/10/26 3:24:00
10-26 01:24 crawler.settings INFO     helpme accept post info
10-26 01:24 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 01:24 crawler.settings INFO     helpme accepted arguments:零次方,342623198902145713,SQC92017102000001,15121016778,HM
10-26 01:24 crawler.settings INFO     HM 零次方,342623198902145713 insert into success
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     helpme saved success
10-26 01:24 crawler.settings INFO     HM 零次方,342623198902145713 insert into success
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 01:24 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:41 crawler.settings INFO     hd_token delete old token
10-26 10:41 crawler.settings INFO     hd_tokenwebloan delete old token
10-26 10:41 crawler.settings INFO     hd_token insert a new token 378602986C33446FB2275DC312C4CF1C4D4F5AF8D32226D58868D2B9B5D63472289BD795640D4355A970AB626C662756,effective 2017/10/26 12:40:55
10-26 10:41 crawler.settings INFO     hd_tokenwebloan insert a new token AD5D6C04214342819168CF3FED6BB50183CEDBDEDF483DDCA6D4A10BE591CCBC01F17FB7109A42B095DEA1C4CAB6B20D,effective 2017/10/26 12:40:55
10-26 10:41 crawler.settings INFO     helpme accept post info
10-26 10:41 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 10:41 crawler.settings INFO     helpme accepted arguments:零次方,342623198902145713,SQC92017102000001,15121016778,HM
10-26 10:41 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 10:41 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 10:41 crawler.settings INFO     HM 零次方,342623198902145713 insert into success
10-26 10:41 crawler.settings INFO     EMW005运营商实名认证
10-26 10:41 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:41 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:42 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:42 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:42 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:42 django.request ERROR    Internal Server Error: /hd-helpme/
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\core\handlers\exception.py", line 41, in inner
    response = get_response(request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "F:\crawler\appCrawler\views\helpMe.py", line 99, in post
    res = hdsave.main()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 436, in main
    if i.get("retcode") == -1:
AttributeError: 'list' object has no attribute 'get'
10-26 10:44 crawler.settings INFO     helpme accept post info
10-26 10:44 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 10:44 crawler.settings INFO     helpme accepted arguments:李清晨,342623198902145713,SQC92017102000001,15121016778,HM
10-26 10:44 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 10:44 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 10:44 crawler.settings INFO     HM 李清晨,342623198902145713 insert into success
10-26 10:44 crawler.settings INFO     EMW005运营商实名认证
10-26 10:44 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:44 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:44 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:44 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:44 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:44 django.request ERROR    Internal Server Error: /hd-helpme/
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\core\handlers\exception.py", line 41, in inner
    response = get_response(request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "F:\crawler\appCrawler\views\helpMe.py", line 99, in post
    res = hdsave.main()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 436, in main
    for code,msg in adict.items():
AttributeError: 'list' object has no attribute 'items'
10-26 10:46 crawler.settings INFO     helpme accept post info
10-26 10:46 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 10:46 crawler.settings INFO     helpme accepted arguments:李清晨,342623198902145713,SQC92017102000001,15121016778,HM
10-26 10:46 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 10:46 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 10:46 crawler.settings INFO     HM 李清晨,342623198902145713 insert into success
10-26 10:46 crawler.settings INFO     EMW005运营商实名认证
10-26 10:46 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:46 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:46 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:46 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:46 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:46 crawler.settings INFO     adict =>
10-26 10:46 django.request ERROR    Internal Server Error: /hd-helpme/
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\core\handlers\exception.py", line 41, in inner
    response = get_response(request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "F:\crawler\appCrawler\views\helpMe.py", line 99, in post
    res = hdsave.main()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 437, in main
    for code,msg in adict.items():
AttributeError: 'list' object has no attribute 'items'
10-26 10:48 crawler.settings INFO     helpme accept post info
10-26 10:48 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 10:48 crawler.settings INFO     helpme accepted arguments:李章秋,342623198902145713,SQC92017102000001,15121016778,HM
10-26 10:48 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 10:48 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 10:48 crawler.settings INFO     HM 李章秋,342623198902145713 insert into success
10-26 10:48 crawler.settings INFO     EMW005运营商实名认证
10-26 10:48 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:48 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:48 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:48 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:48 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:48 crawler.settings INFO     adict =>
10-26 10:48 crawler.settings INFO     adict =>
10-26 10:48 crawler.settings INFO     adict =>
10-26 10:48 crawler.settings INFO     adict =>
10-26 10:48 crawler.settings INFO     helpMe success complete
10-26 10:53 crawler.settings INFO     helpme accept post info
10-26 10:53 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 10:53 crawler.settings INFO     helpme accepted arguments:邱少云,342623198902145713,SQC92017102000001,15121016778,HM
10-26 10:53 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 10:53 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 10:53 crawler.settings INFO     HM 邱少云,342623198902145713 insert into success
10-26 10:53 crawler.settings INFO     EMW005运营商实名认证
10-26 10:53 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:53 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:53 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:53 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:53 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:53 crawler.settings INFO     helpMe success complete
10-26 10:55 crawler.settings INFO     helpme accept post info
10-26 10:55 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 10:55 crawler.settings INFO     helpme accepted arguments:李明旺,342623198902145713,SQC92017102000001,15121016778,HM
10-26 10:55 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 10:55 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 10:55 crawler.settings INFO     HM 李明旺,342623198902145713 insert into success
10-26 10:55 crawler.settings INFO     EMW005运营商实名认证
10-26 10:55 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:55 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:55 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:55 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:55 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 10:55 crawler.settings INFO     helpMe success complete
10-26 11:04 crawler.settings INFO     helpme accept post info
10-26 11:04 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 11:04 crawler.settings INFO     helpme accepted arguments:白线程,342623198902145713,SQC92017102000001,15121016778,HM
10-26 11:04 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 11:04 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 11:04 crawler.settings INFO     HM 白线程,342623198902145713 insert into success
10-26 11:04 crawler.settings INFO     EMW005运营商实名认证
10-26 11:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:04 crawler.settings INFO     helpme succeed
10-26 11:09 crawler.settings INFO     helpme accept post info
10-26 11:09 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-26 11:09 crawler.settings INFO     helpme accepted arguments:郝乐,342623198902145713,SQC92017102000001,15121016778,HM
10-26 11:09 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 11:09 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-26 11:09 crawler.settings INFO     HM 郝乐,342623198902145713 insert into success
10-26 11:09 crawler.settings INFO     EMW005运营商实名认证
10-26 11:09 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:09 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:09 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:09 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:09 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-26 11:09 crawler.settings INFO     helpme succeed
10-26 11:09 crawler.settings INFO     HM 郝乐,342623198902145713 insert into success
10-26 11:09 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 11:11 crawler.settings INFO     SSD 郝乐巴,342623198902145713 insert into success
10-26 11:11 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-26 11:12 django.request ERROR    Internal Server Error: /hd-phone-rz/
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1406, "Data too long for column 'ctime' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 315, in hd_operator_rz
    ssd.save(using="ssd")
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1406, "Data too long for column 'ctime' at row 1")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\core\handlers\exception.py", line 41, in inner
    response = get_response(request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "F:\python3\lib\site-packages\django\core\handlers\base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "F:\python3\lib\site-packages\django\views\generic\base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "F:\crawler\appCrawler\views\SSDViewNew.py", line 77, in post
    res = hd.hd_operator_rz()
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 317, in hd_operator_rz
    logger.error(exc_info=True)
TypeError: error() missing 1 required positional argument: 'msg'
10-26 15:57 crawler.settings INFO     EMW005运营商实名认证
10-26 16:00 crawler.settings INFO     EMW005运营商实名认证
10-27 22:09 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 135, in hd_blackList
    token = self._get_newtoken("hd_token")
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 102, in _get_newtoken
    token = hd_token.objects.using("ssd").values("token").desc()
AttributeError: 'QuerySet' object has no attribute 'desc'
10-27 22:09 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 135, in hd_blackList
    token = self._get_newtoken("hd_token")
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 102, in _get_newtoken
    token = hd_token.objects.using("ssd").values("token").desc()
AttributeError: 'QuerySet' object has no attribute 'desc'
10-27 22:10 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 22:12 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 136, in hd_blackList
    token = self._get_newtoken("hd_token")
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 104, in _get_newtoken
    token = hd_token.objects.using(Coalesce("ssd").values("token"))
  File "F:\python3\lib\site-packages\django\db\models\functions\base.py", line 48, in __init__
    raise ValueError('Coalesce must take at least two expressions')
ValueError: Coalesce must take at least two expressions
10-27 22:14 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 136, in hd_blackList
    token = self._get_newtoken("hd_token")
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 104, in _get_newtoken
    token = hd_token.objects.using("ssd").values(Coalesce("token","id").desc())
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 697, in values
    clone = self._values(*fields, **expressions)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 692, in _values
    clone.query.set_values(fields)
  File "F:\python3\lib\site-packages\django\db\models\sql\query.py", line 1896, in set_values
    self.add_fields(field_names, True)
  File "F:\python3\lib\site-packages\django\db\models\sql\query.py", line 1645, in add_fields
    name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
AttributeError: 'OrderBy' object has no attribute 'split'
10-27 22:16 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 136, in hd_blackList
    token = self._get_newtoken("hd_token")
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 104, in _get_newtoken
    token = hd_token.objects.using("ssd").values("token").order_by("id".desc())
AttributeError: 'str' object has no attribute 'desc'
10-27 22:18 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 22:20 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 22:28 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 23:18 crawler.settings INFO     hd_token delete old token
10-27 23:18 crawler.settings INFO     hd_tokenwebloan delete old token
10-27 23:18 crawler.settings INFO     hd_token insert a new token 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A,effective 2017/10/28 1:18:21
10-27 23:18 crawler.settings INFO     hd_tokenwebloan insert a new token CDF99B7CC6734FDBA1F3FC69F2CFC797BA2F2E6DE03FDD15A8FA21DAD353551F1C03C6C163EF4ED28B8085A5524358D8,effective 2017/10/28 1:18:21
10-27 23:19 crawler.settings INFO     helpme accept post info
10-27 23:19 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-27 23:19 crawler.settings INFO     helpme accepted arguments:万明,342623198902145713,SQC92017102000001,15121016778,HM
10-27 23:19 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:19 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 23:19 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:19 crawler.settings ERROR    EMR012 =>
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 176, in hd_blackList_detail
    EMR012 = res.get("RESULTS")[4].get("DATA")
TypeError: 'NoneType' object is not subscriptable
10-27 23:19 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:19 crawler.settings INFO     HM 万明,342623198902145713 insert into success
10-27 23:19 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:19 crawler.settings INFO     EMW005运营商实名认证
10-27 23:19 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:19 crawler.settings ERROR    EMR002 =>
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 316, in hd_webloan
    self._hd_webloan(cycle)
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 324, in _hd_webloan
    EMR002 = res.get("RESULTS")[0]
TypeError: 'NoneType' object is not subscriptable
10-27 23:19 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:19 crawler.settings ERROR    hd_webloandetail=>
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 397, in hd_webloandetail
    self._hd_webloandetail_oneCycle(c)
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 371, in _hd_webloandetail_oneCycle
    EMR002 = res.get("RESULTS")[0]
TypeError: 'NoneType' object is not subscriptable
10-27 23:19 crawler.settings INFO     helpme succeed
10-27 23:24 crawler.settings INFO     helpme accept post info
10-27 23:24 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-27 23:24 crawler.settings INFO     helpme accepted arguments:万明,342623198902145713,SQC92017102000001,15121016778,HM
10-27 23:24 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:24 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 23:24 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:24 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-27 23:24 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:24 crawler.settings INFO     HM 万明,342623198902145713 insert into success
10-27 23:24 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:24 crawler.settings INFO     EMW005运营商实名认证
10-27 23:24 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:24 crawler.settings ERROR    EMR002 =>
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 317, in hd_webloan
    self._hd_webloan(cycle)
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 328, in _hd_webloan
    register_num = len(EMR002_data)
TypeError: object of type 'NoneType' has no len()
10-27 23:24 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:24 crawler.settings ERROR    hd_webloandetail=>
Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 398, in hd_webloandetail
    self._hd_webloandetail_oneCycle(c)
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 375, in _hd_webloandetail_oneCycle
    times = len(EMR002_data)
TypeError: object of type 'NoneType' has no len()
10-27 23:24 crawler.settings INFO     helpme succeed
10-27 23:30 crawler.settings INFO     helpme accept post info
10-27 23:30 crawler.settings INFO     helpme instantiation hd_interface and hd saving
10-27 23:30 crawler.settings INFO     helpme accepted arguments:万明,342623198902145713,SQC92017102000001,15121016778,HM
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:30 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     EMR012 逾期平台详情查询 success
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:30 crawler.settings INFO     HM 万明,342623198902145713 insert into success
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:30 crawler.settings INFO     EMW005运营商实名认证
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15121016778
10-27 23:30 crawler.settings INFO     helpme succeed
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:30 crawler.settings INFO     SSD 郝乐巴,342623198902145713 insert into success
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:30 crawler.settings INFO     EMW025 黑名单模糊汇总查询 success
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:30 crawler.settings ERROR    hd_webloandetail=>
Traceback (most recent call last):
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.OperationalError: (1054, "Unknown column 'p_phone' in 'field list'")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 398, in hd_webloandetail
    self._hd_webloandetail_oneCycle(c)
  File "F:\crawler\appCrawler\ForwRecord\hd_storage.py", line 393, in _hd_webloandetail_oneCycle
    ssd.save(using=self.SSDDB)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "F:\python3\lib\site-packages\django\db\models\base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "F:\python3\lib\site-packages\django\db\models\manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "F:\python3\lib\site-packages\django\db\models\query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "F:\python3\lib\site-packages\django\db\models\sql\compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "F:\python3\lib\site-packages\django\utils\six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "F:\python3\lib\site-packages\django\db\backends\utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "F:\python3\lib\site-packages\django\db\backends\mysql\base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 247, in execute
    res = self._query(query)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "F:\python3\lib\site-packages\MySQLdb\cursors.py", line 374, in _do_query
    db.query(q)
  File "F:\python3\lib\site-packages\MySQLdb\connections.py", line 292, in query
    _mysql.connection.query(self, query)
django.db.utils.OperationalError: (1054, "Unknown column 'p_phone' in 'field list'")
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_tokenwebloan
10-27 23:36 crawler.settings INFO     get newtoken 68317810A30343548933B61453F1D0E6C29C869482F12C1226CCAB1E9E89C2981DA5B797E68E41C0BAC52D5576DAA46A from hd_token
10-27 23:36 crawler.settings INFO     EMW005运营商实名认证
11-02 17:53 crawler.settings INFO     get newtoken 4C926D0372BD42EC9DB54806BA8AEACA1677FAB833F812231E94FDC58D8F0741ADF9550D262347769C3D6B2A9FE6CD3C from hd_token
11-02 17:53 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 153, in hd_blackList
    queryRes = bsd_n.objects.filter(p_person_id=self.idcard).lastest('id')
AttributeError: 'QuerySet' object has no attribute 'lastest'
11-02 17:54 crawler.settings INFO     get newtoken 4C926D0372BD42EC9DB54806BA8AEACA1677FAB833F812231E94FDC58D8F0741ADF9550D262347769C3D6B2A9FE6CD3C from hd_token
11-02 17:54 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 153, in hd_blackList
    queryRes = bsd_n.objects.filter(p_person_id=self.idcard).lastest('request_id')
AttributeError: 'QuerySet' object has no attribute 'lastest'
11-02 17:55 crawler.settings INFO     get newtoken 4C926D0372BD42EC9DB54806BA8AEACA1677FAB833F812231E94FDC58D8F0741ADF9550D262347769C3D6B2A9FE6CD3C from hd_token
11-02 17:55 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 154, in hd_blackList
    queryRes = bsd_n.objects.filter(p_person_id=self.idcard).latest('request_id')
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 558, in latest
    return self._earliest_or_latest(field_name=field_name, direction="-")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 552, in _earliest_or_latest
    return obj.get()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 380, in get
    self.model._meta.object_name
appCrawler.models.DoesNotExist: bsd_hy_blacklist_hit_summary matching query does not exist.
11-02 17:58 crawler.settings INFO     get newtoken 4C926D0372BD42EC9DB54806BA8AEACA1677FAB833F812231E94FDC58D8F0741ADF9550D262347769C3D6B2A9FE6CD3C from hd_token
11-02 17:58 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 154, in hd_blackList
    queryRes = bsd_n.objects.filter(p_person_id=self.idcard).latest('request_id')
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 558, in latest
    return self._earliest_or_latest(field_name=field_name, direction="-")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 552, in _earliest_or_latest
    return obj.get()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 380, in get
    self.model._meta.object_name
appCrawler.models.DoesNotExist: bsd_hy_blacklist_hit_summary matching query does not exist.
11-02 18:00 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:00 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    if self.detect_expire(queryRes.ctime):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 133, in detect_expire
    detected_time = self.datetime_timestamp(dt)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 120, in datetime_timestamp
    time.strptime(dt, '%Y-%m-%d %H:%M:%S')
  File "/usr/lib/python3.6/_strptime.py", line 559, in _strptime_time
    tt = _strptime(data_string, format)[0]
  File "/usr/lib/python3.6/_strptime.py", line 329, in _strptime
    raise TypeError(msg.format(index, type(arg)))
TypeError: strptime() argument 0 must be str, not <class 'datetime.datetime'>
11-02 18:04 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:04 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    if self.detect_expire(queryRes.ctime):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 133, in detect_expire
    detected_time = self.datetime_timestamp(dt)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 120, in datetime_timestamp
    time.strptime(dt, '%Y-%m-%d %H:%M:%S')
  File "/usr/lib/python3.6/_strptime.py", line 559, in _strptime_time
    tt = _strptime(data_string, format)[0]
  File "/usr/lib/python3.6/_strptime.py", line 329, in _strptime
    raise TypeError(msg.format(index, type(arg)))
TypeError: strptime() argument 0 must be str, not <class 'datetime.datetime'>
11-02 18:09 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:09 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    if self.detect_expire(queryRes.ctime):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 133, in detect_expire
    detected_time = self.datetime_timestamp(dt)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 121, in datetime_timestamp
    s = time.mktime(time.strptime(dt, '%Y-%m-%d %H:%M:%S'))
  File "/usr/lib/python3.6/_strptime.py", line 559, in _strptime_time
    tt = _strptime(data_string, format)[0]
  File "/usr/lib/python3.6/_strptime.py", line 329, in _strptime
    raise TypeError(msg.format(index, type(arg)))
TypeError: strptime() argument 0 must be str, not <class 'datetime.datetime'>
11-02 18:10 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:10 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): opensdk.emay.cn
11-02 18:10 crawler.settings INFO     EMW025 榛妯＄姹绘ヨ success
11-02 18:14 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:14 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): opensdk.emay.cn
11-02 18:14 crawler.settings INFO     EMW025 榛妯＄姹绘ヨ success
11-02 18:14 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:14 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    queryRes = bsd_n.objects.using(self.HMDB).filter(p_person_id=self.idcard).lastest('id')
AttributeError: 'QuerySet' object has no attribute 'lastest'
11-02 18:15 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:15 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    queryRes = bsd_n.objects.using(self.HMDB).filter(p_person_id=self.idcard).lastest('request_id')
AttributeError: 'QuerySet' object has no attribute 'lastest'
11-02 18:15 crawler.settings INFO     get newtoken BF6BB72191A34B2DB635F76A21A1BA5BF0D8A04DD0AEBB3547217B2738FED85FACD4614037FA488A910AE7A7CAE3A5F1 from hd_token
11-02 18:15 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 168, in hd_blackList
    bsd.black_hit_count = parseRes.get("WDHMDCOUNT","")
AttributeError: 'bsd_hy_blacklist_hit_summary' object has no attribute 'get'
11-03 06:05 django.request WARNING  Not Found: /
11-03 06:05 django.request WARNING  Not Found: /favicon.ico
11-03 06:07 django.request WARNING  Not Found: /
11-03 06:07 django.request WARNING  Not Found: /favicon.ico
11-03 06:13 django.request WARNING  Not Found: /
11-03 06:15 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:15 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-03 06:15 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 142, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 91, in create_connection
    raise err
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 81, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 578, in urlopen
    chunked=chunked)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 362, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 167, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 151, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.HTTPConnection object at 0x7f48f609b588>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 403, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 623, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 281, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=empty&ACCESS_TOKEN=41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f48f609b588>: Failed to establish a new connection: [Errno 111] Connection refused',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 183, in hd_blackList
    parseRes = self.req_hdInterface(self.hdBlackListUrl, data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 76, in req_hdInterface
    getRes = requests.get(complete_url,"")
  File "/usr/lib/python3/dist-packages/requests/api.py", line 71, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 57, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 585, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 465, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=empty&ACCESS_TOKEN=41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f48f609b588>: Failed to establish a new connection: [Errno 111] Connection refused',)))
11-03 06:16 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:16 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-03 06:16 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 142, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 91, in create_connection
    raise err
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 81, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 578, in urlopen
    chunked=chunked)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 362, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 167, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 151, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.HTTPConnection object at 0x7fa6d42ae390>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 403, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 623, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 281, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=empty&ACCESS_TOKEN=41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7fa6d42ae390>: Failed to establish a new connection: [Errno 111] Connection refused',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 183, in hd_blackList
    parseRes = self.req_hdInterface(self.hdBlackListUrl, data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 76, in req_hdInterface
    getRes = requests.get(complete_url,"")
  File "/usr/lib/python3/dist-packages/requests/api.py", line 71, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 57, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 585, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 465, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=empty&ACCESS_TOKEN=41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7fa6d42ae390>: Failed to establish a new connection: [Errno 111] Connection refused',)))
11-03 06:19 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:19 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    queryRes = bsd_n.objects.using(self.HMDB).filter(p_person_id=self.idcard).latest('request_id')
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 558, in latest
    return self._earliest_or_latest(field_name=field_name, direction="-")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 552, in _earliest_or_latest
    return obj.get()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 380, in get
    self.model._meta.object_name
appCrawler.models.DoesNotExist: bsd_hy_blacklist_hit_summary matching query does not exist.
11-03 06:23 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:23 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    queryRes = bsd_n.objects.using(self.HMDB).filter(p_person_id=self.idcard).lastest('request_id')
AttributeError: 'QuerySet' object has no attribute 'lastest'
11-03 06:23 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:23 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    queryRes = bsd_n.objects.using(self.HMDB).filter(p_person_id=self.idcard).latest('request_id')
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 558, in latest
    return self._earliest_or_latest(field_name=field_name, direction="-")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 552, in _earliest_or_latest
    return obj.get()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 380, in get
    self.model._meta.object_name
appCrawler.models.DoesNotExist: bsd_hy_blacklist_hit_summary matching query does not exist.
11-03 06:26 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:26 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 157, in hd_blackList
    queryRes = bsd_n.objects.using(self.HMDB).filter(p_person_id=self.idcard).latest('request_id')
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 558, in latest
    return self._earliest_or_latest(field_name=field_name, direction="-")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 552, in _earliest_or_latest
    return obj.get()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 380, in get
    self.model._meta.object_name
appCrawler.models.DoesNotExist: bsd_hy_blacklist_hit_summary matching query does not exist.
11-03 06:27 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:27 crawler.settings INFO     EMW025 榛妯＄姹绘ヨ success
11-03 06:28 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:28 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 156, in hd_blackList
    queryRes = bsd_n.objects.using(self.HMDB).filter(p_person_id=self.idcard).latest('request_id')
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 558, in latest
    return self._earliest_or_latest(field_name=field_name, direction="-")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 552, in _earliest_or_latest
    return obj.get()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 380, in get
    self.model._meta.object_name
appCrawler.models.DoesNotExist: bsd_hy_blacklist_hit_summary matching query does not exist.
11-03 06:28 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:28 crawler.settings INFO     EMW025 榛妯＄姹绘ヨ success
11-03 06:32 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:32 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 160, in hd_blackList
    if self.detect_expire(queryRes.ctime):
AttributeError: 'QuerySet' object has no attribute 'ctime'
11-03 06:33 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:33 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 160, in hd_blackList
    if self.detect_expire(queryRes.ctime):
AttributeError: 'QuerySet' object has no attribute 'ctime'
11-03 06:33 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:33 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 160, in hd_blackList
    if self.detect_expire(queryRes.ctime):
AttributeError: 'QuerySet' object has no attribute 'ctime'
11-03 06:42 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:43 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 159, in hd_blackList
    print("queryRes --------------->",queryRes[:1][0])
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 289, in __getitem__
    return list(qs)[0]
IndexError: list index out of range
11-03 06:43 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:43 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 160, in hd_blackList
    if self.detect_expire(queryRes.ctime):
AttributeError: 'QuerySet' object has no attribute 'ctime'
11-03 06:44 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:44 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 160, in hd_blackList
    if self.detect_expire(queryRes.ctime):
AttributeError: 'QuerySet' object has no attribute 'ctime'
11-03 06:50 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:50 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-03 06:50 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 142, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 91, in create_connection
    raise err
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 81, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 578, in urlopen
    chunked=chunked)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 362, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 167, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 151, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.HTTPConnection object at 0x7f89f4b4d860>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 403, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 623, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 281, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=34032319881202081200&ACCESS_TOKEN=41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f89f4b4d860>: Failed to establish a new connection: [Errno 111] Connection refused',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 161, in hd_blackList
    parseRes = self.req_hdInterface(self.hdBlackListUrl, data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 76, in req_hdInterface
    getRes = requests.get(complete_url,"")
  File "/usr/lib/python3/dist-packages/requests/api.py", line 71, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 57, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 585, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 465, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=34032319881202081200&ACCESS_TOKEN=41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7f89f4b4d860>: Failed to establish a new connection: [Errno 111] Connection refused',)))
11-03 06:51 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:51 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 168, in hd_blackList
    "WDHMDCOUNT":parseRes.black_hit_count,
AttributeError: 'str' object has no attribute 'black_hit_count'
11-03 06:52 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:52 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/hd_storage.py", line 171, in hd_blackList
    bsd.black_hit_count = parseRes.get("WDHMDCOUNT","")
AttributeError: 'str' object has no attribute 'get'
11-03 06:55 crawler.settings INFO     get newtoken 41403E659B8F4D808D1A7BD2E3DF401107C86AD90FA082B23B8BA8F0F6B528A054622491055B430BB7454B80139D5E2F from hd_token
11-03 06:55 crawler.settings INFO     EMW025 榛妯＄姹绘ヨ success
11-08 15:51 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 15:56 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:16 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:18 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:19 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:19 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:20 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:20 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:21 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:23 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:23 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:24 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:25 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:25 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:25 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:26 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:26 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:27 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:27 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:27 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:29 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:29 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:30 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:31 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:33 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:33 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:38 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:45 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:46 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:47 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:48 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:55 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:56 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 16:57 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:02 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:04 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:04 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:07 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:33 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:35 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:38 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:42 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:42 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-08 17:58 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 09:26 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 09:26 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 09:45 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 10:19 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 10:22 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 10:22 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 10:22 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 10:22 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-09 10:23 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 08:55 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 08:55 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 49, in main
    storage_res = self.storage(parsed)
AttributeError: 'zhengxin91' object has no attribute 'storage'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 17, in post
    if zx.main(py_obj):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 60, in main
    raise Exception("error is ok2")
Exception: error is ok2
11-10 08:56 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 08:56 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 122, in storage
    raise Exception("Unexpected parsed struct")
Exception: Unexpected parsed struct
11-10 08:56 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.ProgrammingError: (1146, "Table 'crawler.log_zhengxin91_crawler' doesn't exist")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 57, in main
    c.save()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.ProgrammingError: (1146, "Table 'crawler.log_zhengxin91_crawler' doesn't exist")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 17, in post
    if zx.main(py_obj):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 60, in main
    raise Exception("error is ok2")
Exception: error is ok2
11-10 08:58 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 08:58 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 123, in storage
    raise Exception("Unexpected parsed struct")
Exception: Unexpected parsed struct
11-10 08:58 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.ProgrammingError: (1146, "Table 'crawler.log_zhengxin91_crawler' doesn't exist")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 57, in main
    c.save()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.ProgrammingError: (1146, "Table 'crawler.log_zhengxin91_crawler' doesn't exist")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 17, in post
    if zx.main(py_obj):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 60, in main
    raise Exception("error is ok2")
Exception: error is ok2
11-10 13:35 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 13:35 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 118, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 133, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 13:35 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 13:35 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 138, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 155, in _store_ori
    zx.flag = data[self.rec_flag]
AttributeError: 'zhengxin91' object has no attribute 'rec_flag'
11-10 13:35 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 14:47 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 14:47 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 118, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 133, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 14:47 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 14:47 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 138, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 156, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 14:47 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 14:51 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 14:51 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 135, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 14:51 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 14:51 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 140, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 158, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 14:51 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 14:54 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 14:54 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 137, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 14:54 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 14:54 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 142, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 160, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 14:54 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 14:55 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 14:55 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 137, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 14:55 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 14:55 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 142, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 160, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 14:55 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 14:56 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 14:56 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 137, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 14:56 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 14:56 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 142, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 160, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 14:56 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 14:58 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 14:58 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 138, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 14:58 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 14:58 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 143, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 161, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 14:58 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 14:59 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 14:59 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 138, in _decode
    py_obj = json.loads(str(de_res))
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 14:59 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 14:59 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 143, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 161, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 14:59 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:01 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:01 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 138, in _decode
    py_obj = json.loads(str(de_res))
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 15:01 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 15:01 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 143, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 161, in _store_ori
    zx.status_code = parsed[0]
IndexError: tuple index out of range
11-10 15:01 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:07 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:07 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 138, in _decode
    py_obj = json.loads(str(de_res))
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 15:07 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 15:07 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 143, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 157, in _store_ori
    assert len(parsed) == 2
AssertionError
11-10 15:07 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:08 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:08 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 136, in _decode
    de_res = base64.b64decode(extracted).encode("utf-8")
AttributeError: 'bytes' object has no attribute 'encode'
11-10 15:08 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 15:08 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 143, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 157, in _store_ori
    assert len(parsed) == 2
AssertionError
11-10 15:08 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:10 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:10 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 135, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 15:10 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 15:10 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 140, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 154, in _store_ori
    assert len(parsed) == 2
AssertionError
11-10 15:10 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:12 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:12 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 136, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 15:12 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 15:12 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 141, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 155, in _store_ori
    assert len(parsed) == 2
AssertionError
11-10 15:12 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:14 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:14 crawler.settings ERROR    parse error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 120, in parse
    py_obj = self._decode(extracted)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 137, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 354, in loads
    return _default_decoder.decode(s)
  File "/usr/lib/python3.6/json/decoder.py", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File "/usr/lib/python3.6/json/decoder.py", line 357, in raw_decode
    raise JSONDecodeError("Expecting value", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
11-10 15:14 crawler.settings ERROR    01|SRV000000001|01||01|01||9999|57O757uf5Y+R55Sf5byC5bi45p+l6K+i6K+35rGC5aSx6LSl|
11-10 15:14 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 142, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 156, in _store_ori
    assert len(parsed) == 2
AssertionError
11-10 15:14 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:15 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:15 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 141, in storage
    flag = data[self.rec_flag]
AttributeError: 'zhengxin91' object has no attribute 'rec_flag'
11-10 15:15 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:16 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:16 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 145, in storage
    raise Exception(f"Unexpected flag {flag}")
Exception: Unexpected flag HM
11-10 15:16 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:19 crawler.settings ERROR    query_storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 36, in main
    if data[self.FLAG] == self.BSDFLAG and self.hm_not_expire(data):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 85, in hm_not_expire
    if len(queryRes) == 0:
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 232, in __len__
    self._fetch_all()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1118, in _fetch_all
    self._result_cache = list(self._iterable_class(self))
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 53, in __iter__
    results = compiler.execute_sql(chunked_fetch=self.chunked_fetch)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 871, in execute_sql
    sql, params = self.as_sql()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 423, in as_sql
    extra_select, order_by, group_by = self.pre_sql_setup()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 47, in pre_sql_setup
    order_by = self.get_order_by()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 298, in get_order_by
    field, self.query.get_meta(), default_order=asc))
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 601, in find_ordering_name
    field, targets, alias, joins, path, opts = self._setup_joins(pieces, opts, alias)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 634, in _setup_joins
    pieces, opts, alias)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/query.py", line 1417, in setup_joins
    names, opts, allow_many, fail_on_missing=True)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/query.py", line 1352, in names_to_path
    "Choices are: %s" % (name, ", ".join(available)))
django.core.exceptions.FieldError: Cannot resolve keyword 'request_id' into field. Choices are: arrears_amount, borrow_amount, borrow_status, borrow_type, company_code, contract_data, ctime, ctime_stamp, err_msg, id, idcard, loan_period, name, repay_state, status_code, trx_no
11-10 15:19 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:44 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 15:44 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.OperationalError: (1048, "Column 'ctime_stamp' cannot be null")

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 141, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 175, in _store_bsd
    zx.save(using=self.HMDB)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 106, in execute
    six.reraise(utils.IntegrityError, utils.IntegrityError(*tuple(e.args)), sys.exc_info()[2])
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.IntegrityError: (1048, "Column 'ctime_stamp' cannot be null")
11-10 15:44 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 15:55 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 16:22 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 16:25 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 16:34 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 16:38 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 16:40 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 16:40 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 143, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 161, in _store_ori
    zx.flag = data[self.FLAG]
KeyError: 'flag'
11-10 16:40 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 16:54 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 16:54 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 144, in storage
    self._store_ori(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 165, in _store_ori
    zx.flag = data[self.FLAG]
KeyError: 'flag'
11-10 16:54 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:02 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:02 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 147, in storage
    raise Exception("synchronous interface return symbolic error code")
Exception: synchronous interface return symbolic error code
11-10 17:02 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:11 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:11 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 148, in storage
    raise Exception("synchronous interface return symbolic error code")
Exception: synchronous interface return symbolic error code
11-10 17:11 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:13 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:13 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 149, in storage
    raise Exception("synchronous interface return symbolic error code")
Exception: synchronous interface return symbolic error code
11-10 17:13 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:16 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:16 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 150, in storage
    raise Exception("synchronous interface return symbolic error code")
Exception: synchronous interface return symbolic error code
11-10 17:16 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:26 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:26 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1406, "Data too long for column 'err_msg' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 154, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 189, in _store_bsd
    zx.save(using=self.HMDB)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1406, "Data too long for column 'err_msg' at row 1")
11-10 17:26 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:28 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:29 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1406, "Data too long for column 'err_msg' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 154, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 190, in _store_bsd
    zx.save(using=self.HMDB)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1406, "Data too long for column 'err_msg' at row 1")
11-10 17:29 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:29 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:29 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1406, "Data too long for column 'err_msg' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 154, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 190, in _store_bsd
    zx.save(using=self.HMDB)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1406, "Data too long for column 'err_msg' at row 1")
11-10 17:29 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 17:34 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 17:42 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:03 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:03 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1264, "Out of range value for column 'contract_data' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 156, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 203, in _store_bsd
    zx.save(using=self.HMDB)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1264, "Out of range value for column 'contract_data' at row 1")
11-10 18:03 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 18:16 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:16 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.DataError: (1264, "Out of range value for column 'contract_data' at row 1")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 156, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 203, in _store_bsd
    zx.save(using=self.HMDB)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.DataError: (1264, "Out of range value for column 'contract_data' at row 1")
11-10 18:16 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 18:19 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:21 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:22 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:33 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:33 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 156, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 201, in _store_bsd
    company_code = loan_dict.get("companyCode",self.GOTNONE),
TypeError: 'zhengxin91_content' object is not callable
11-10 18:33 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-10 18:37 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:38 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 18:39 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-10 21:07 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-11 04:43 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-11 04:45 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-13 09:21 crawler.settings INFO     hm data not expire
11-13 09:21 crawler.settings ERROR    query_storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 41, in main
    self.update_existed(hd_data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 98, in update_existed
    trx_no = zx.objects.using(self.HMDB).filter(idcard=data[self.rec_idcard]).order_by("-id")[:1][0].trx_no
NameError: name 'data' is not defined
11-13 09:21 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-13 09:23 crawler.settings INFO     hm data not expire
11-13 09:23 crawler.settings ERROR    query_storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 41, in main
    self.update_existed(hd_data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 102, in update_existed
    self._store_ori(parsed,hm_data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 173, in _store_ori
    assert len(parsed) == 2
AssertionError
11-13 09:23 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-13 09:24 crawler.settings INFO     hm data not expire
11-13 09:24 crawler.settings ERROR    query_storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 41, in main
    self.update_existed(hd_data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 102, in update_existed
    self._store_ori(parsed,hm_data)
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/ForwRecord/zhengxin91.py", line 173, in _store_ori
    assert len(parsed) == 2
AssertionError
11-13 09:24 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/work_code/crawler/appCrawler/views/zhengxin91.py", line 20, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-13 09:35 crawler.settings INFO     hm data not expire
11-13 09:45 crawler.settings INFO     hm data not expire
11-16 09:34 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/zhengxin91.py", line 23, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-16 09:39 crawler.settings ERROR    update error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 27, in main
    self.update_existed(data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 82, in update_existed
    parsed = (self.Error.EXISTED_ERROR,self.Error.EXISTED_HINT)
AttributeError: type object 'Error' has no attribute 'EXISTED_HINT'
11-16 09:39 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/zhengxin91.py", line 23, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-16 09:44 crawler.settings ERROR    query_storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 27, in main
    self.update_existed(data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 81, in update_existed
    parsed = (self.Error.EXISTED_ERROR,self.Error.EXISTED_HINT)
AttributeError: type object 'Error' has no attribute 'EXISTED_HINT'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 32, in main
    raise Exception(self.Error.EXISTED_FAIL_HINT)
Exception: exsited data is not expire;It failed in update
11-16 09:44 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/zhengxin91.py", line 23, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-16 09:53 crawler.settings ERROR    request error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 87, in request
    user_data = self._encode_query(data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 101, in _encode_query
    to_base64 = self.zhengxin_encode(post_data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/manager.py", line 69, in zhengxin_encode
    encoded = base64.b64encode(marshaled).decode()
  File "/usr/lib/python3.6/base64.py", line 58, in b64encode
    encoded = binascii.b2a_base64(s, newline=False)
TypeError: a bytes-like object is required, not 'str'
11-16 09:53 crawler.settings ERROR    {'flag': 'HM', 'realName': '存', 'idCard': '32020219790403401X'}
11-16 09:53 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/zhengxin91.py", line 23, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-16 09:57 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-16 10:25 crawler.settings INFO     verify query success
11-16 10:27 crawler.settings INFO     verify query success
11-16 10:27 crawler.settings INFO     exsited data is not expire;ctime was updated only
11-16 10:29 crawler.settings INFO     verify query success
11-16 10:29 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-16 10:29 crawler.settings INFO     request success
11-16 10:29 crawler.settings INFO     storage success
11-16 10:30 crawler.settings INFO     verify query success
11-16 10:30 crawler.settings INFO     exsited data is not expire;ctime was updated only
11-16 14:13 crawler.settings ERROR    receive data is illegal
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 39, in verify
    assert isinstance(rcv,str)
AssertionError
11-16 14:13 crawler.settings ERROR    Exception error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 39, in verify
    assert isinstance(rcv,str)
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 14, in main
    self.verify(rcv,log_db)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 47, in verify
    raise Exception("verify receive => False")
Exception: verify receive => False

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.ProgrammingError: (1146, "Table 'crawler.zhengxin91_share_log' doesn't exist")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/zhengxin91.py", line 34, in post
    resp,state = zx.main(request.body)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 33, in main
    self.save_log(self.WRONG, log_db)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/manager.py", line 130, in save_log
    table.save()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 808, in save
    force_update=force_update, update_fields=update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 838, in save_base
    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 924, in _save_table
    result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/base.py", line 963, in _do_insert
    using=using, raw=raw)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/manager.py", line 85, in manager_method
    return getattr(self.get_queryset(), name)(*args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1076, in _insert
    return query.get_compiler(using=using).execute_sql(return_id)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 1107, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.ProgrammingError: (1146, "Table 'crawler.zhengxin91_share_log' doesn't exist")
11-16 14:20 crawler.settings ERROR    receive data is illegal
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 40, in verify
    assert isinstance(rcv,str)
AssertionError
11-16 14:20 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 40, in verify
    assert isinstance(rcv,str)
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 14, in main
    self.verify(rcv,log_db)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 48, in verify
    raise Exception("verify receive => False")
Exception: verify receive => False
11-16 14:20 crawler.settings ERROR    Exception error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/zhengxin91.py", line 39, in post
    mock_data = e.except_resp(request.body)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos_failed.py", line 5, in except_resp
    to_list = rcv.split("|")
TypeError: a bytes-like object is required, not 'str'
11-16 14:36 crawler.settings INFO     verify receive success
11-16 14:36 crawler.settings INFO     parse success
11-16 14:36 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
_mysql_exceptions.ProgrammingError: (1146, "Table 'crawler.zhengxin91_share_ori_data' doesn't exist")

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 18, in main
    if self.not_expire(ZhengXin91ShareOriData,parsed):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/manager.py", line 101, in not_expire
    if len(queryRes) == 0:
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 232, in __len__
    self._fetch_all()
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 1118, in _fetch_all
    self._result_cache = list(self._iterable_class(self))
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 53, in __iter__
    results = compiler.execute_sql(chunked_fetch=self.chunked_fetch)
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 894, in execute_sql
    raise original_exception
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/sql/compiler.py", line 884, in execute_sql
    cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 80, in execute
    return super(CursorDebugWrapper, self).execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/utils.py", line 94, in __exit__
    six.reraise(dj_exc_type, dj_exc_value, traceback)
  File "/usr/local/lib/python3.6/dist-packages/django/utils/six.py", line 685, in reraise
    raise value.with_traceback(tb)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/utils.py", line 65, in execute
    return self.cursor.execute(sql, params)
  File "/usr/local/lib/python3.6/dist-packages/django/db/backends/mysql/base.py", line 101, in execute
    return self.cursor.execute(query, args)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 250, in execute
    self.errorhandler(self, exc, value)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 50, in defaulterrorhandler
    raise errorvalue
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 247, in execute
    res = self._query(query)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 411, in _query
    rowcount = self._do_query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/cursors.py", line 374, in _do_query
    db.query(q)
  File "/usr/local/lib/python3.6/dist-packages/MySQLdb/connections.py", line 277, in query
    _mysql.connection.query(self, query)
django.db.utils.ProgrammingError: (1146, "Table 'crawler.zhengxin91_share_ori_data' doesn't exist")
11-16 15:18 crawler.settings INFO     verify receive success
11-16 15:18 crawler.settings INFO     parse success
11-16 15:18 crawler.settings INFO     query success
11-16 15:18 crawler.settings INFO     trans success
11-16 15:18 crawler.settings INFO     save cache success
11-16 15:37 crawler.settings INFO     verify receive success
11-16 15:37 crawler.settings INFO     parse success
11-16 15:46 crawler.settings INFO     verify receive success
11-16 15:46 crawler.settings INFO     parse success
11-16 16:41 crawler.settings INFO     verify receive success
11-16 16:41 crawler.settings INFO     parse success
11-16 16:48 crawler.settings INFO     verify receive success
11-16 16:48 crawler.settings INFO     parse success
11-16 16:48 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 23, in main
    return extracted.res,True
AttributeError: 'str' object has no attribute 'res'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 27, in main
    raise Exception(self.Error.EXISTED_FAIL_HINT)
Exception: exsited data is not expire;It failed in update
11-16 16:48 crawler.settings INFO     query failed,using default data
11-16 16:50 crawler.settings INFO     verify receive success
11-16 16:50 crawler.settings INFO     parse success
11-16 16:50 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 20, in main
    extracted,_ = self.extract_not_expire(parsed)
ValueError: too many values to unpack (expected 2)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 27, in main
    raise Exception(self.Error.EXISTED_FAIL_HINT)
Exception: exsited data is not expire;It failed in update
11-16 16:50 crawler.settings INFO     query failed,using default data
11-16 16:52 crawler.settings INFO     verify receive success
11-16 16:52 crawler.settings INFO     parse success
11-16 16:52 crawler.settings INFO     query table success
11-16 16:53 crawler.settings INFO     verify receive success
11-16 16:53 crawler.settings INFO     parse success
11-16 16:53 crawler.settings INFO     query table success
11-16 17:01 crawler.settings INFO     verify receive success
11-16 17:01 crawler.settings INFO     parse success
11-16 17:01 crawler.settings INFO     ZhengXin91ShareLog 1 saved
11-16 17:01 crawler.settings INFO     query table success
11-16 17:17 crawler.settings INFO     verify receive success
11-16 17:17 crawler.settings INFO     parse success
11-16 17:17 crawler.settings INFO     query success
11-16 17:17 crawler.settings INFO     trans success
11-16 17:17 crawler.settings INFO     save cache success
11-16 17:17 crawler.settings INFO     ZhengXin91ShareLog 1 saved
11-16 17:17 crawler.settings INFO     query table success
11-16 17:27 crawler.settings INFO     verify receive success
11-16 17:27 crawler.settings INFO     parse success
11-16 17:27 crawler.settings INFO     ZhengXin91ShareLog successfully saved
11-16 17:27 crawler.settings INFO     query table success
11-16 17:33 crawler.settings INFO     verify receive success
11-16 17:33 crawler.settings INFO     parse success
11-16 17:33 crawler.settings INFO     query success
11-16 17:33 crawler.settings INFO     trans success
11-16 17:33 crawler.settings INFO     save cache success
11-16 17:33 crawler.settings INFO     ZhengXin91ShareLog successfully saved
11-16 17:33 crawler.settings INFO     query table success
11-16 17:35 crawler.settings INFO     verify receive success
11-16 17:35 crawler.settings INFO     parse success
11-16 17:35 crawler.settings INFO     ZhengXin91ShareLog successfully saved
11-16 17:35 crawler.settings INFO     response from cache, succeed!
11-16 17:35 crawler.settings INFO     query table success
11-17 10:52 crawler.settings INFO     verify receive success
11-17 10:52 crawler.settings INFO     parse success
11-17 10:52 crawler.settings INFO     ZhengXin91ShareLog unsuccessfully saved
11-17 10:52 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 16, in main
    self.verify_meanful_rcv(parsed, log_db)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 72, in verify_meanful_rcv
    raise Exception("notEmptyDict => False")
Exception: notEmptyDict => False
11-17 10:52 crawler.settings INFO     query failed,using default data
11-17 10:53 crawler.settings INFO     verify receive success
11-17 10:53 crawler.settings INFO     parse success
11-17 10:53 crawler.settings INFO     ZhengXin91ShareLog unsuccessfully saved
11-17 10:53 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 16, in main
    self.verify_meanful_rcv(parsed, log_db)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 72, in verify_meanful_rcv
    raise Exception("notEmptyDict => False")
Exception: notEmptyDict => False
11-17 10:53 crawler.settings INFO     query failed,using default data
11-17 11:00 crawler.settings INFO     verify receive success
11-17 11:00 crawler.settings INFO     parse success
11-17 11:00 crawler.settings ERROR    not Empty dict
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/manager.py", line 89, in notEmptyDict
    assert adict[k] != ""
AssertionError
11-17 11:00 crawler.settings INFO     ZhengXin91ShareLog unsuccessfully saved
11-17 11:00 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 16, in main
    self.verify_meanful_rcv(parsed, log_db)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 72, in verify_meanful_rcv
    raise Exception("notEmptyDict => False")
Exception: notEmptyDict => False
11-17 11:00 crawler.settings INFO     query failed,using default data
11-17 11:01 crawler.settings INFO     verify receive success
11-17 11:01 crawler.settings INFO     parse success
11-17 11:01 crawler.settings ERROR    not Empty dict
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/manager.py", line 89, in notEmptyDict
    assert adict[k] != ""
AssertionError
11-17 11:01 crawler.settings INFO     ZhengXin91ShareLog unsuccessfully saved
11-17 11:01 crawler.settings ERROR    shareLoanInfo91 failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 16, in main
    self.verify_meanful_rcv(parsed, log_db)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/shared_infos.py", line 72, in verify_meanful_rcv
    raise Exception("notEmptyDict => False")
Exception: notEmptyDict => False
11-17 11:01 crawler.settings INFO     query failed,using default data
11-17 11:02 crawler.settings INFO     verify receive success
11-17 11:02 crawler.settings INFO     parse success
11-17 11:02 crawler.settings INFO     query success
11-17 11:02 crawler.settings INFO     trans success
11-17 11:02 crawler.settings INFO     save cache success
11-17 11:02 crawler.settings INFO     ZhengXin91ShareLog successfully saved
11-17 11:02 crawler.settings INFO     query table success
11-20 14:07 crawler.settings INFO     verify receive success
11-20 14:07 crawler.settings INFO     parse success
11-20 14:07 crawler.settings INFO     ZhengXin91ShareLog successfully saved
11-20 14:07 crawler.settings INFO     response from cache, succeed!
11-20 14:07 crawler.settings INFO     query table success
11-20 14:10 crawler.settings INFO     verify query success
11-20 14:10 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-20 14:10 crawler.settings INFO     request success
11-20 14:10 crawler.settings ERROR    _decode
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 106, in _decode
    py_obj = json.loads(de_res)
  File "/usr/lib/python3.6/json/__init__.py", line 348, in loads
    'not {!r}'.format(s.__class__.__name__))
TypeError: the JSON object must be str, bytes or bytearray, not 'dict'
11-20 14:10 crawler.settings INFO     storage success
11-20 14:10 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-20 14:10 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-20 14:13 crawler.settings INFO     verify query success
11-20 14:13 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): test.91zhengxin.com
11-20 14:13 crawler.settings INFO     request success
11-20 14:13 crawler.settings INFO     storage success
11-20 14:13 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-20 14:13 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-23 14:42 crawler.settings INFO     verify query success
11-23 14:42 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-23 14:42 crawler.settings INFO     request success
11-23 14:42 crawler.settings INFO     storage success
11-23 14:42 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-23 14:42 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-23 14:42 crawler.settings INFO     verify query success
11-23 14:42 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-23 14:42 crawler.settings INFO     request success
11-23 14:42 crawler.settings INFO     storage success
11-23 14:42 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-23 14:42 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-23 14:59 crawler.settings INFO     verify query success
11-23 14:59 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-23 14:59 crawler.settings INFO     request success
11-23 14:59 crawler.settings INFO     this is receive response
11-23 14:59 crawler.settings INFO     01|SRV000000001|01|2003|01|01|eyJ0cnhObyI6ImI1ZTFkMTU1MGM5NzQwZWM4ZmFhNzk4MmY5ZjI5NTkwIiwibG9hbkluZm9zIjpbXX0=|0000|5p+l6K+i5oiQ5Yqf|
11-23 14:59 crawler.settings INFO     this is parsed
11-23 14:59 crawler.settings INFO     storage success
11-23 14:59 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-23 14:59 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-23 15:35 crawler.settings INFO     verify query success
11-23 15:35 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-23 15:35 crawler.settings INFO     request success
11-23 15:35 crawler.settings INFO     this is receive response
11-23 15:35 crawler.settings INFO     01|SRV000000001|01|2003|01|01|eyJ0cnhObyI6IjgwZWRiOTM4ODg3MDRkNjA5Y2RkNTU2NmI4YTc4OWIyIiwibG9hbkluZm9zIjpbXX0=|0000|5p+l6K+i5oiQ5Yqf|
11-23 15:35 crawler.settings INFO     this is parsed
11-23 15:35 crawler.settings INFO     {"trxNo": "80edb93888704d609cdd5566b8a789b2", "loanInfos": []}
11-23 15:35 crawler.settings INFO     the message of zhengxin91 responsed is empty
11-23 15:35 crawler.settings ERROR    storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 122, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 186, in _store_bsd
    borrow_type=loan.get(core.BORROWSTATE, 0),
UnboundLocalError: local variable 'loan' referenced before assignment
11-23 15:35 crawler.settings INFO     zhengxin91_crawler_log unsuccessfully saved
11-23 15:35 crawler.settings ERROR    query_storage error
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 122, in storage
    self._store_bsd(parsed,data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 186, in _store_bsd
    borrow_type=loan.get(core.BORROWSTATE, 0),
UnboundLocalError: local variable 'loan' referenced before assignment

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 30, in main
    self.storage(parsed,data,c)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/zhengxin91/infos.py", line 132, in storage
    raise Exception("storage error")
Exception: storage error
11-23 15:35 crawler.settings ERROR    zhengxin91 error traceback
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/zhengxin91.py", line 24, in post
    raise Exception("zhengxin91 query or storatrge error")
Exception: zhengxin91 query or storatrge error
11-23 15:38 crawler.settings INFO     verify query success
11-23 15:38 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-23 15:38 crawler.settings INFO     request success
11-23 15:38 crawler.settings INFO     this is receive response
11-23 15:38 crawler.settings INFO     01|SRV000000001|01|2003|01|01|eyJ0cnhObyI6ImE5NjE3OWUxMmY5YzQzNzNiNzVhNjJkMmRkNmIwYzg0IiwibG9hbkluZm9zIjpbXX0=|0000|5p+l6K+i5oiQ5Yqf|
11-23 15:38 crawler.settings INFO     this is parsed
11-23 15:38 crawler.settings INFO     {"trxNo": "a96179e12f9c4373b75a62d2dd6b0c84", "loanInfos": []}
11-23 15:38 crawler.settings INFO     the message of zhengxin91 responsed is empty
11-23 15:38 crawler.settings INFO     storage success
11-23 15:38 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-23 15:38 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-24 16:05 crawler.settings INFO     verify query success
11-24 16:05 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-24 16:05 crawler.settings INFO     request success
11-24 16:05 crawler.settings INFO     this is receive response
11-24 16:05 crawler.settings INFO     01|SRV000000001|01|2003|01|01|eyJ0cnhObyI6Ijg1NzViMTMzY2UyOTQ5ZTdiZWQwNzg3MWMyMTEwMDMzIiwibG9hbkluZm9zIjpbeyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6MSwiYm9ycm93QW1vdW50IjowLCJjb250cmFjdERhdGUiOjE1MDIxODY4MjgwMDAsImxvYW5QZXJpb2QiOjAsInJlcGF5U3RhdGUiOjAsImFycmVhcnNBbW91bnQiOjAsImNvbXBhbnlDb2RlIjoiUDJQMDk3NzgwIn0seyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6NiwiYm9ycm93QW1vdW50IjowLCJjb250cmFjdERhdGUiOjE0OTkxNjExMjQwMDAsImxvYW5QZXJpb2QiOjAsInJlcGF5U3RhdGUiOjAsImFycmVhcnNBbW91bnQiOjAsImNvbXBhbnlDb2RlIjoiUDJQNjQ2MDUzIn0seyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6NiwiYm9ycm93QW1vdW50IjowLCJjb250cmFjdERhdGUiOjE1MDE4NDQ3NDAwMDAsImxvYW5QZXJpb2QiOjAsInJlcGF5U3RhdGUiOjAsImFycmVhcnNBbW91bnQiOjAsImNvbXBhbnlDb2RlIjoiUDJQNjQ2MDUzIn0seyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6NiwiYm9ycm93QW1vdW50IjowLCJjb250cmFjdERhdGUiOjE0OTU3ODU1MDMwMDAsImxvYW5QZXJpb2QiOjAsInJlcGF5U3RhdGUiOjAsImFycmVhcnNBbW91bnQiOjAsImNvbXBhbnlDb2RlIjoiUDJQNjQ2MDUzIn0seyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6MiwiYm9ycm93QW1vdW50IjotNiwiY29udHJhY3REYXRlIjoxNDk3NjI4ODAwMDAwLCJsb2FuUGVyaW9kIjoxLCJyZXBheVN0YXRlIjoyLCJhcnJlYXJzQW1vdW50IjoyMDM5NjcwMDAsImNvbXBhbnlDb2RlIjoiUDJQNDU5MDQ3In0seyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6NiwiYm9ycm93QW1vdW50IjowLCJjb250cmFjdERhdGUiOjE1MDY0MTkzNjEwMDAsImxvYW5QZXJpb2QiOjAsInJlcGF5U3RhdGUiOjAsImFycmVhcnNBbW91bnQiOjAsImNvbXBhbnlDb2RlIjoiUDJQNjQ2MDUzIn1dfQ==|0000|5p+l6K+i5oiQ5Yqf|
11-24 16:05 crawler.settings INFO     this is parsed
11-24 16:05 crawler.settings INFO     {"trxNo": "8575b133ce2949e7bed07871c2110033", "loanInfos": [{"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1502186828000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P097780"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1499161124000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P646053"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1501844740000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P646053"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1495785503000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P646053"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1497628800000, "loanPeriod": 1, "repayState": 2, "arrearsAmount": 203967000, "companyCode": "P2P459047"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1506419361000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P646053"}]}
11-24 16:05 crawler.settings INFO     storage success
11-24 16:05 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-24 16:05 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-24 16:07 crawler.settings INFO     verify query success
11-24 16:07 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-24 16:07 crawler.settings INFO     request success
11-24 16:07 crawler.settings INFO     this is receive response
11-24 16:07 crawler.settings INFO     01|SRV000000001|01|2003|01|01|eyJ0cnhObyI6ImY2MGQ0MDJhNWQ4NTQ1YzhiOGJlMTk3ZjIxMjE3NGY5IiwibG9hbkluZm9zIjpbeyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6NiwiYm9ycm93QW1vdW50IjowLCJjb250cmFjdERhdGUiOjE1MDI4NDUxNzkwMDAsImxvYW5QZXJpb2QiOjAsInJlcGF5U3RhdGUiOjAsImFycmVhcnNBbW91bnQiOjAsImNvbXBhbnlDb2RlIjoiUDJQNDUzMDg3In0seyJib3Jyb3dUeXBlIjoxLCJib3Jyb3dTdGF0ZSI6MiwiYm9ycm93QW1vdW50IjotNiwiY29udHJhY3REYXRlIjoxNTAxNDczMzY3MDAwLCJsb2FuUGVyaW9kIjoxLCJyZXBheVN0YXRlIjo5LCJhcnJlYXJzQW1vdW50IjowLCJjb21wYW55Q29kZSI6IlAyUDM2MDQ5OSJ9LHsiYm9ycm93VHlwZSI6MSwiYm9ycm93U3RhdGUiOjYsImJvcnJvd0Ftb3VudCI6MCwiY29udHJhY3REYXRlIjoxNTA2NTg0ODIxMDAwLCJsb2FuUGVyaW9kIjowLCJyZXBheVN0YXRlIjowLCJhcnJlYXJzQW1vdW50IjowLCJjb21wYW55Q29kZSI6IlAyUDQ1MzA4NyJ9LHsiYm9ycm93VHlwZSI6MSwiYm9ycm93U3RhdGUiOjYsImJvcnJvd0Ftb3VudCI6MCwiY29udHJhY3REYXRlIjoxNTA5MjUzOTk0MDAwLCJsb2FuUGVyaW9kIjowLCJyZXBheVN0YXRlIjowLCJhcnJlYXJzQW1vdW50IjowLCJjb21wYW55Q29kZSI6IlAyUDQ1MzA4NyJ9LHsiYm9ycm93VHlwZSI6MSwiYm9ycm93U3RhdGUiOjEsImJvcnJvd0Ftb3VudCI6MCwiY29udHJhY3REYXRlIjoxNTAyMDk1ODY5MDAwLCJsb2FuUGVyaW9kIjowLCJyZXBheVN0YXRlIjowLCJhcnJlYXJzQW1vdW50IjowLCJjb21wYW55Q29kZSI6IlAyUDg2NDUxOSJ9LHsiYm9ycm93VHlwZSI6MSwiYm9ycm93U3RhdGUiOjIsImJvcnJvd0Ftb3VudCI6LTYsImNvbnRyYWN0RGF0ZSI6MTUwMzE5ODk3MTAwMCwibG9hblBlcmlvZCI6MSwicmVwYXlTdGF0ZSI6MiwiYXJyZWFyc0Ftb3VudCI6MzY5NjgzMDAwLCJjb21wYW55Q29kZSI6IlAyUDM2MDQ5OSJ9LHsiYm9ycm93VHlwZSI6MSwiYm9ycm93U3RhdGUiOjYsImJvcnJvd0Ftb3VudCI6MCwiY29udHJhY3REYXRlIjoxNDk2NjY0MjE3MDAwLCJsb2FuUGVyaW9kIjowLCJyZXBheVN0YXRlIjowLCJhcnJlYXJzQW1vdW50IjowLCJjb21wYW55Q29kZSI6IlAyUDQ1MzA4NyJ9XX0=|0000|5p+l6K+i5oiQ5Yqf|
11-24 16:07 crawler.settings INFO     this is parsed
11-24 16:07 crawler.settings INFO     {"trxNo": "f60d402a5d8545c8b8be197f212174f9", "loanInfos": [{"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1502845179000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P453087"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1501473367000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P360499"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1506584821000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P453087"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1509253994000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P453087"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1502095869000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P864519"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1503198971000, "loanPeriod": 1, "repayState": 2, "arrearsAmount": 369683000, "companyCode": "P2P360499"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1496664217000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P453087"}]}
11-24 16:07 crawler.settings INFO     storage success
11-24 16:07 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-24 16:07 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-24 16:07 crawler.settings INFO     verify query success
11-24 16:07 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-24 16:07 crawler.settings INFO     request success
11-24 16:07 crawler.settings INFO     this is receive response
11-24 16:07 crawler.settings INFO     01|SRV000000001|01|2003|01|01|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|0000|5p+l6K+i5oiQ5Yqf|
11-24 16:07 crawler.settings INFO     this is parsed
11-24 16:07 crawler.settings INFO     {"trxNo": "028d747a216b47f79c2731a23a023b30", "loanInfos": [{"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1484865602000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P842188"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1502052792000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P842188"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1496880000000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P059596"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1485561600000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P884665"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1507766400000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P884665"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1497312000000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P059596"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1493510400000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P059596"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -5, "contractDate": 1498233600000, "loanPeriod": 14, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P969169"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1500422400000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P059596"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -5, "contractDate": 1488683155000, "loanPeriod": 1, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P954108"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1493521936000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P842188"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1499441395000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P842188"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1484916904000, "loanPeriod": 1, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P954108"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1483228800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P371225"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1501977297000, "loanPeriod": 1, "repayState": 4, "arrearsAmount": 418700000, "companyCode": "P2P488368"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": -28800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P714959"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1505867970000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P842188"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1486259565000, "loanPeriod": 1, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P954108"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -5, "contractDate": 1491990600000, "loanPeriod": 1, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P954108"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 0, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P906061"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1486252800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P059596"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1496076423000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P842188"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1490817481000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P842188"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1498003200000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P884665"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1486339200000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P884665"}]}
11-24 16:07 crawler.settings INFO     storage success
11-24 16:07 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-24 16:07 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-24 16:07 crawler.settings INFO     verify query success
11-24 16:07 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-24 16:07 crawler.settings INFO     request success
11-24 16:07 crawler.settings INFO     this is receive response
11-24 16:07 crawler.settings INFO     01|SRV000000001|01|2003|01|01|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|0000|5p+l6K+i5oiQ5Yqf|
11-24 16:07 crawler.settings INFO     this is parsed
11-24 16:07 crawler.settings INFO     {"trxNo": "a5dd8f7197ed4335ada7c78d8b4b1fb2", "loanInfos": [{"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1508716800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1506038400000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1430409600000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P836993"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1446307200000, "loanPeriod": 2, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P181868"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -5, "contractDate": 1497916800000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1501459200000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P139215"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -5, "contractDate": 1496275200000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -5, "contractDate": 1496361600000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P139215"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1443628800000, "loanPeriod": 2, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P537814"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1503273600000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1448899200000, "loanPeriod": 2, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P537814"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -5, "contractDate": 1500854400000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1504828800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1456761600000, "loanPeriod": 2, "repayState": 1, "arrearsAmount": 0, "companyCode": "P2P181868"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1500163200000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P139215"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1504137600000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1498176000000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P139215"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1499644800000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1507698511000, "loanPeriod": 1, "repayState": 1, "arrearsAmount": 0, "companyCode": "P2P362985"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1503014400000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1503619200000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1483228800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P940794"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1500681600000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P139215"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1506096000000, "loanPeriod": 1, "repayState": 2, "arrearsAmount": 112329000, "companyCode": "P2P076625"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1454256000000, "loanPeriod": 2, "repayState": 1, "arrearsAmount": 0, "companyCode": "P2P181868"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -7, "contractDate": 1441036800000, "loanPeriod": 3, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P181868"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1495843200000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P139215"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": -28800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P076625"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1495756800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1503360000000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": 3, "contractDate": 1433088000000, "loanPeriod": 48, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P803424"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1507593600000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P899045"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1497398400000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P139215"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1494720000000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P899045"}]}
11-24 16:07 crawler.settings INFO     storage success
11-24 16:07 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-24 16:07 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-24 16:08 crawler.settings INFO     verify query success
11-24 16:08 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): service.91zhengxin.com
11-24 16:08 crawler.settings INFO     request success
11-24 16:08 crawler.settings INFO     this is receive response
11-24 16:08 crawler.settings INFO     01|SRV000000001|01|2003|01|01|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|0000|5p+l6K+i5oiQ5Yqf|
11-24 16:08 crawler.settings INFO     this is parsed
11-24 16:08 crawler.settings INFO     {"trxNo": "c98e8c31899b46968d0f288c3d56f707", "loanInfos": [{"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1500698012000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P389757"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1506567860000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P389757"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1500595200000, "loanPeriod": 1, "repayState": 9, "arrearsAmount": 0, "companyCode": "P2P765096"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1499558400000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P765096"}, {"borrowType": 1, "borrowState": 2, "borrowAmount": -6, "contractDate": 1502409600000, "loanPeriod": 1, "repayState": 3, "arrearsAmount": 296800000, "companyCode": "P2P765096"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1483228800000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P932157"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1503463156000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P539058"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1497850209000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P539058"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1484537817000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P539058"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1477958400000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P932157"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1497844043000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P389757"}, {"borrowType": 1, "borrowState": 6, "borrowAmount": 0, "contractDate": 1480308651000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P539058"}, {"borrowType": 1, "borrowState": 1, "borrowAmount": 0, "contractDate": 1503654456000, "loanPeriod": 0, "repayState": 0, "arrearsAmount": 0, "companyCode": "P2P389757"}]}
11-24 16:08 crawler.settings INFO     storage success
11-24 16:08 crawler.settings INFO     zhengxin91_crawler_log successfully saved
11-24 16:08 crawler.settings INFO     Accepted_Query: zhengxin91storage
11-28 07:44 django.request ERROR    Internal Server Error: /hd-webloan/
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/exception.py", line 41, in inner
    response = get_response(request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/ssd.py", line 38, in post
    res1 = hd.ssd_webloan()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 574, in ssd_webloan
    res = self._EMD008_integrate_interface(self.SSDWEBLOANCYCLE)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 104, in _EMD008_integrate_interface
    token = self._get_newtoken("hd_tokenwebloan")
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 96, in _get_newtoken
    token = hd_token.objects.using("ssd").values("token").order_by("-id")[0].get("token","")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 289, in __getitem__
    return list(qs)[0]
IndexError: list index out of range
11-28 07:56 django.request ERROR    Internal Server Error: /hd-webloan/
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/exception.py", line 41, in inner
    response = get_response(request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/ssd.py", line 38, in post
    res1 = hd.ssd_webloan()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 574, in ssd_webloan
    res = self._EMD008_integrate_interface(self.SSDWEBLOANCYCLE)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 104, in _EMD008_integrate_interface
    token = self._get_newtoken("hd_tokenwebloan")
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 96, in _get_newtoken
    token = hd_token.objects.using("ssd").values("token").order_by("-id")[0].get("token","")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 289, in __getitem__
    return list(qs)[0]
IndexError: list index out of range
11-28 08:01 django.request ERROR    Internal Server Error: /hd-webloan/
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/exception.py", line 41, in inner
    response = get_response(request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/ssd.py", line 38, in post
    res1 = hd.ssd_webloan()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 574, in ssd_webloan
    res = self._EMD008_integrate_interface(self.SSDWEBLOANCYCLE)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 104, in _EMD008_integrate_interface
    token = self._get_newtoken("hd_tokenwebloan")
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 96, in _get_newtoken
    token = hd_token.objects.using("ssd").values("token").order_by("-id")[0].get("token","")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 289, in __getitem__
    return list(qs)[0]
IndexError: list index out of range
11-28 08:07 django.request ERROR    Internal Server Error: /hd-webloan/
Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/exception.py", line 41, in inner
    response = get_response(request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 68, in view
    return self.dispatch(request, *args, **kwargs)
  File "/usr/local/lib/python3.6/dist-packages/django/views/generic/base.py", line 88, in dispatch
    return handler(request, *args, **kwargs)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/ssd.py", line 38, in post
    res1 = hd.ssd_webloan()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 574, in ssd_webloan
    res = self._EMD008_integrate_interface(self.SSDWEBLOANCYCLE)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 104, in _EMD008_integrate_interface
    token = self._get_newtoken("hd_tokenwebloan")
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 96, in _get_newtoken
    token = hd_tokenwebloan.objects.using("ssd").values("token").order_by("-id")[0].get("token","")
  File "/usr/local/lib/python3.6/dist-packages/django/db/models/query.py", line 289, in __getitem__
    return list(qs)[0]
IndexError: list index out of range
11-28 14:02 crawler.settings INFO     get newtoken FA2F0A8CD67F45A9AF22DE3BC3126855D4CAFEB3C51E6F78AD95D40D0E08CA2E74EAB180F6DF4A2681D6D1F2F4801FD3 from hd_tokenwebloan
11-28 14:02 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-28 14:02 crawler.settings INFO     get newtoken FA2F0A8CD67F45A9AF22DE3BC3126855D4CAFEB3C51E6F78AD95D40D0E08CA2E74EAB180F6DF4A2681D6D1F2F4801FD3 from hd_tokenwebloan
11-28 14:02 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-28 14:02 crawler.settings INFO     EMR002 淇¤捶骞冲版敞璇/webloan success
11-28 14:02 crawler.settings INFO     hd_webloandetail started
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:02 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:02 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:02 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:02 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:02 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:02 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:02 crawler.settings INFO     hd_webloandetail end
11-28 14:02 crawler.settings INFO     webloandetail success
11-28 14:02 app_crawler.views.ssd INFO     Accepted_Query EMR004 璐锋剧宠锋℃
11-28 14:04 crawler.settings INFO     get newtoken FA2F0A8CD67F45A9AF22DE3BC3126855D4CAFEB3C51E6F78AD95D40D0E08CA2E74EAB180F6DF4A2681D6D1F2F4801FD3 from hd_tokenwebloan
11-28 14:04 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-28 14:04 crawler.settings INFO     EMR002 淇¤捶骞冲版敞璇/webloan success
11-28 14:04 crawler.settings INFO     hd_webloandetail started
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:04 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:04 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:04 crawler.settings INFO     hd_webloandetail end
11-28 14:04 crawler.settings INFO     webloandetail success
11-28 14:04 app_crawler.views.ssd INFO     Accepted_Query EMR004 璐锋剧宠锋℃
11-28 14:16 crawler.settings INFO     get newtoken FA2F0A8CD67F45A9AF22DE3BC3126855D4CAFEB3C51E6F78AD95D40D0E08CA2E74EAB180F6DF4A2681D6D1F2F4801FD3 from hd_tokenwebloan
11-28 14:16 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-28 14:16 crawler.settings INFO     related data have existed
11-28 14:16 crawler.settings INFO     related data have existed
11-28 14:16 crawler.settings INFO     related data have existed
11-28 14:16 crawler.settings INFO     related data have existed
11-28 14:16 crawler.settings INFO     related data have existed
11-28 14:16 crawler.settings INFO     EMR002 淇¤捶骞冲版敞璇/webloan success
11-28 14:16 crawler.settings INFO     hd_webloandetail started
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:16 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:16 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:16 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:16 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail started
11-28 14:16 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-28 14:16 crawler.settings INFO     HM _hd_webloandetail ended
11-28 14:16 crawler.settings INFO     hd_webloandetail end
11-28 14:16 crawler.settings INFO     webloandetail success
11-28 14:16 app_crawler.views.ssd INFO     Accepted_Query EMR004 璐锋剧宠锋℃
11-29 11:38 crawler.settings INFO     emd008 parse succeed,CODE,PHONE,PROVINCE,CITY
11-29 11:38 crawler.settings ERROR    hd emd008 loan detail failed
11-29 11:38 django.request WARNING  Not Found: /favicon.ico
11-29 11:43 crawler.settings INFO     emd008 parse succeed,CODE,PHONE,PROVINCE,CITY
11-29 11:43 crawler.settings ERROR    hd emd008 loan detail failed
11-29 11:45 crawler.settings INFO     emd008 parse succeed,CODE,PHONE,PROVINCE,CITY
11-29 11:45 crawler.settings INFO     phone---> PHONE
11-29 11:45 crawler.settings INFO     emd_id --> 5
11-29 11:45 crawler.settings ERROR    hd emd008 loan detail failed
11-29 11:49 crawler.settings INFO     emd008 parse succeed,CODE,PHONE,PROVINCE,CITY
11-29 11:49 crawler.settings INFO     phone---> PHONE;name -->13131314
11-29 11:49 crawler.settings INFO     emd_id --> 7
11-29 11:49 crawler.settings ERROR    hd emd008 loan detail failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 42, in store_detail
    setattr(EMD008Detail,"cycle",r["CYCLE"])
TypeError: string indices must be integers
11-29 12:40 crawler.settings ERROR    emd008 parse failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 54, in parse_data
    self.code = code["CODE"]
TypeError: string indices must be integers
11-29 12:40 django.request ERROR    Internal Server Error: /test/emd008/
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 54, in parse_data
    self.code = code["CODE"]
TypeError: string indices must be integers

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/exception.py", line 41, in inner
    response = get_response(request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/view_test.py", line 5, in emd008_test
    infos_emd008_test()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/case_test/infos_emd008.py", line 5, in infos_emd008_test
    e.main()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 21, in main
    self.parse_data()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 62, in parse_data
    raise Exception(err)
Exception: emd008 parse failed
11-29 12:49 crawler.settings INFO     emd008 parse succeed,200,1111111111,浜,浜
11-29 12:49 crawler.settings INFO     store in HDEMD008Manager id is 9
11-29 12:49 crawler.settings ERROR    hd emd008 loan detail failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 42, in store_detail
    func(r["DATA"])
TypeError: EMR002() missing 1 required positional argument: 'data'
11-29 12:54 crawler.settings INFO     emd008 parse succeed,200,1111111111,浜,浜
11-29 12:54 crawler.settings INFO     store in HDEMD008Manager id is 11
11-29 12:54 crawler.settings ERROR    hd emd008 loan detail failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 45, in store_detail
    func(r["DATA"])
TypeError: EMR002() missing 1 required positional argument: 'data'
11-29 14:02 crawler.settings INFO     emd008 parse succeed,200,1111111111,浜,浜
11-29 14:02 crawler.settings INFO     store in HDEMD008Manager id is 13
11-29 14:02 crawler.settings ERROR    hd emd008 loan detail failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 45, in store_detail
    func(r["DATA"])
TypeError: EMR002() missing 1 required positional argument: 'data'
11-29 14:04 crawler.settings INFO     emd008 parse succeed,200,1111111111,浜,浜
11-29 14:04 crawler.settings INFO     store in HDEMD008Manager id is 15
11-29 14:04 crawler.settings INFO     emd008:emr002 save succeed
11-29 14:04 crawler.settings INFO     emd008:emr004 save succeed
11-29 14:04 crawler.settings INFO     emd008:emr007 save succeed
11-29 14:04 crawler.settings INFO     emd008:emr009 save succeed
11-29 14:04 crawler.settings INFO     emd008:emr012 save succeed
11-29 14:04 crawler.settings INFO     emd008:emr013 save succeed
11-29 14:04 crawler.settings INFO     hd emd008 loan detail successfully stored
11-29 14:17 crawler.settings INFO     emd008 parse succeed,200,1111111111,浜,浜
11-29 14:17 crawler.settings INFO     store in HDEMD008Manager id is 17
11-29 14:17 crawler.settings INFO     emd008:emr002 save succeed
11-29 14:17 crawler.settings INFO     emd008:emr004 save succeed
11-29 14:17 crawler.settings INFO     emd008:emr007 save succeed
11-29 14:17 crawler.settings INFO     emd008:emr009 save succeed
11-29 14:17 crawler.settings INFO     emd008:emr012 save succeed
11-29 14:17 crawler.settings INFO     emd008:emr013 save succeed
11-29 14:17 crawler.settings INFO     hd emd008 loan detail successfully stored
11-29 14:18 crawler.settings ERROR    emd008 parse failed
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 51, in parse_data
    assert len(self.data) == 5
AssertionError
11-29 14:18 django.request ERROR    Internal Server Error: /test/emd008/
Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 51, in parse_data
    assert len(self.data) == 5
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/exception.py", line 41, in inner
    response = get_response(request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 187, in _get_response
    response = self.process_exception_by_middleware(e, request)
  File "/usr/local/lib/python3.6/dist-packages/django/core/handlers/base.py", line 185, in _get_response
    response = wrapped_callback(request, *callback_args, **callback_kwargs)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/views/view_test.py", line 5, in emd008_test
    infos_emd008_test()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/case_test/infos_emd008.py", line 5, in infos_emd008_test
    e.main()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 22, in main
    self.parse_data()
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos_emd008.py", line 60, in parse_data
    raise Exception(err)
Exception: emd008 parse failed
11-29 14:25 crawler.settings INFO     hd emd008 response is {},empty!
11-29 14:25 crawler.settings INFO     store in HDEMD008Manager id is 19
11-29 14:27 crawler.settings INFO     emd008 parse succeed,200,1111111111,浜,浜
11-29 14:27 crawler.settings INFO     store in HDEMD008Manager id is 21
11-29 14:27 crawler.settings INFO     hd emd008 loan details are empty
11-29 15:17 crawler.settings INFO     get newtoken 3C0F0335542F45B5B07C098A7EAB5BD632BED860E9991FFFF96D604887C477020243537FCA06410A86F7A5440D1207C8 from hd_tokenwebloan
11-29 15:17 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-29 15:17 crawler.settings INFO     related data have existed
11-29 15:17 crawler.settings INFO     related data have existed
11-29 15:17 crawler.settings INFO     related data have existed
11-29 15:17 crawler.settings INFO     related data have existed
11-29 15:17 crawler.settings INFO     related data have existed
11-29 15:17 crawler.settings INFO     EMR002 淇¤捶骞冲版敞璇/webloan success
11-29 15:17 crawler.settings INFO     hd_webloandetail started
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail started
11-29 15:17 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail ended
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail started
11-29 15:17 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail ended
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail started
11-29 15:17 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail ended
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail started
11-29 15:17 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail ended
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail started
11-29 15:17 crawler.settings INFO     bsd_loanapplicationnum saved;user phone number=>15857133387
11-29 15:17 crawler.settings INFO     HM _hd_webloandetail ended
11-29 15:17 crawler.settings INFO     hd_webloandetail end
11-29 15:17 crawler.settings INFO     webloandetail success
11-29 15:17 app_crawler.views.ssd INFO     Accepted_Query EMR004 璐锋剧宠锋℃
11-29 15:21 crawler.settings INFO     this is GET
11-29 15:29 django.security.DisallowedHost ERROR    Invalid HTTP_HOST header: 'localhost:8000'. You may need to add 'localhost' to ALLOWED_HOSTS.
11-29 15:29 django.security.DisallowedHost ERROR    Invalid HTTP_HOST header: 'localhost:8000'. You may need to add 'localhost' to ALLOWED_HOSTS.
11-29 15:29 django.request WARNING  Not Found: /
11-29 15:29 django.request WARNING  Not Found: /favicon.ico
11-29 15:30 django.request WARNING  Not Found: /
11-29 15:30 crawler.settings INFO     this is GET
11-29 15:49 django.request WARNING  Not Found: /
11-29 15:49 django.request WARNING  Not Found: /favicon.ico
11-29 15:50 crawler.settings INFO     this is GET
11-29 15:53 crawler.settings INFO     this is GET
11-29 15:57 crawler.settings INFO     get newtoken 39415D2CFE934FB6A621167513C1FD23FCC54BFCBD14C31F6BFF42CAF4FF921AD426A76D41C949618038FFDAF62FBF83 from hd_token
11-29 15:57 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-29 15:57 crawler.settings ERROR    _hd_blackList_judge_before_crawl
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 142, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 91, in create_connection
    raise err
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 81, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 578, in urlopen
    chunked=chunked)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 362, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 167, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 151, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2cb898>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 403, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 623, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 281, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=610324198710123417&ACCESS_TOKEN=39415D2CFE934FB6A621167513C1FD23FCC54BFCBD14C31F6BFF42CAF4FF921AD426A76D41C949618038FFDAF62FBF83& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2cb898>: Failed to establish a new connection: [Errno 111] Connection refused',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 180, in _hd_blackList_judge_before_crawl
    parseRes = self.req_hdInterface(self.hdBlackListUrl, data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 82, in req_hdInterface
    getRes = requests.get(complete_url,"")
  File "/usr/lib/python3/dist-packages/requests/api.py", line 71, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 57, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 585, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 465, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=610324198710123417&ACCESS_TOKEN=39415D2CFE934FB6A621167513C1FD23FCC54BFCBD14C31F6BFF42CAF4FF921AD426A76D41C949618038FFDAF62FBF83& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2cb898>: Failed to establish a new connection: [Errno 111] Connection refused',)))
11-29 15:57 requests.packages.urllib3.connectionpool INFO     Starting new HTTP connection (1): 127.0.0.1
11-29 15:57 crawler.settings ERROR    hd_blackList failed
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 142, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 91, in create_connection
    raise err
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 81, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 578, in urlopen
    chunked=chunked)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 362, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 167, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 151, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2cb898>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 403, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 623, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 281, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=610324198710123417&ACCESS_TOKEN=39415D2CFE934FB6A621167513C1FD23FCC54BFCBD14C31F6BFF42CAF4FF921AD426A76D41C949618038FFDAF62FBF83& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2cb898>: Failed to establish a new connection: [Errno 111] Connection refused',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 180, in _hd_blackList_judge_before_crawl
    parseRes = self.req_hdInterface(self.hdBlackListUrl, data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 82, in req_hdInterface
    getRes = requests.get(complete_url,"")
  File "/usr/lib/python3/dist-packages/requests/api.py", line 71, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 57, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 585, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 465, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=610324198710123417&ACCESS_TOKEN=39415D2CFE934FB6A621167513C1FD23FCC54BFCBD14C31F6BFF42CAF4FF921AD426A76D41C949618038FFDAF62FBF83& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2cb898>: Failed to establish a new connection: [Errno 111] Connection refused',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 142, in _new_conn
    (self.host, self.port), self.timeout, **extra_kw)
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 91, in create_connection
    raise err
  File "/usr/lib/python3/dist-packages/urllib3/util/connection.py", line 81, in create_connection
    sock.connect(sa)
ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 578, in urlopen
    chunked=chunked)
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 362, in _make_request
    conn.request(method, url, **httplib_request_kw)
  File "/usr/lib/python3.6/http/client.py", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File "/usr/lib/python3.6/http/client.py", line 1026, in _send_output
    self.send(msg)
  File "/usr/lib/python3.6/http/client.py", line 964, in send
    self.connect()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 167, in connect
    conn = self._new_conn()
  File "/usr/lib/python3/dist-packages/urllib3/connection.py", line 151, in _new_conn
    self, "Failed to establish a new connection: %s" % e)
requests.packages.urllib3.exceptions.NewConnectionError: <requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2d7b38>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 403, in send
    timeout=timeout
  File "/usr/lib/python3/dist-packages/urllib3/connectionpool.py", line 623, in urlopen
    _stacktrace=sys.exc_info()[2])
  File "/usr/lib/python3/dist-packages/urllib3/util/retry.py", line 281, in increment
    raise MaxRetryError(_pool, url, error or ResponseError(cause))
requests.packages.urllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=610324198710123417&ACCESS_TOKEN=39415D2CFE934FB6A621167513C1FD23FCC54BFCBD14C31F6BFF42CAF4FF921AD426A76D41C949618038FFDAF62FBF83& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2d7b38>: Failed to establish a new connection: [Errno 111] Connection refused',)))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 205, in hd_blackList
    parseRes = self._hd_blackList_judge_before_crawl(data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 191, in _hd_blackList_judge_before_crawl
    parseRes = self.req_hdInterface(self.hdBlackListUrl, data)
  File "/home/zzjack/Desktop/code_work/crawler/app_crawler/storage/huadao/infos.py", line 82, in req_hdInterface
    getRes = requests.get(complete_url,"")
  File "/usr/lib/python3/dist-packages/requests/api.py", line 71, in get
    return request('get', url, params=params, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/api.py", line 57, in request
    return session.request(method=method, url=url, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File "/usr/lib/python3/dist-packages/requests/sessions.py", line 585, in send
    r = adapter.send(request, **kwargs)
  File "/usr/lib/python3/dist-packages/requests/adapters.py", line 465, in send
    raise ProxyError(e, request=request)
requests.exceptions.ProxyError: HTTPConnectionPool(host='127.0.0.1', port=38833): Max retries exceeded with url: http://opensdk.emay.cn:9099/SF_YZ_API/SFService.asmx/Get_EMW_BlackFuzzy_CX?&idcard=610324198710123417&ACCESS_TOKEN=39415D2CFE934FB6A621167513C1FD23FCC54BFCBD14C31F6BFF42CAF4FF921AD426A76D41C949618038FFDAF62FBF83& (Caused by ProxyError('Cannot connect to proxy.', NewConnectionError('<requests.packages.urllib3.connection.HTTPConnection object at 0x7ff0fc2d7b38>: Failed to establish a new connection: [Errno 111] Connection refused',)))
11-29 15:57 app_crawler.views.ssd INFO     Accepted_Query EMW025 榛妯＄姹绘ヨ
